Model,EM Score,F1 Score,Paper Link,Title,github href,paper href,Year,Industry Affiliation,Academia Affiliation,Country
LUKE,90.202,95.379,https://paperswithcode.com/paper/luke-deep-contextualized-entity,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://paperswithcode.com/paper/luke-deep-contextualized-entity#code,https://paperswithcode.com/paper/luke-deep-contextualized-entity/review/?hl=36857,2020,Studio Ousia,"RIKEN, University of Washington, NAIST, National Institute of Informatics",
LUKE,90.2,95.4,https://paperswithcode.com/paper/luke-deep-contextualized-entity,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://paperswithcode.com/paper/luke-deep-contextualized-entity#code,https://paperswithcode.com/paper/luke-deep-contextualized-entity/review/?hl=20507,2020,Studio Ousia,"RIKEN, University of Washington, NAIST, National Institute of Informatics",
XLNet,89.898,95.08,https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining#code,https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining/review/?hl=5561,2019,Google Brain,Carnegie Mellon University,
SpanBERT,88.8,94.6,https://paperswithcode.com/paper/spanbert-improving-pre-training-by,SpanBERT: Improving Pre-training by Representing and Predicting Spans,https://paperswithcode.com/paper/spanbert-improving-pre-training-by#code,https://paperswithcode.com/paper/spanbert-improving-pre-training-by/review/?hl=5968,2019,"Facebook AI Research, AI2","University of Washington, Princeton University",
BERT,87.433,93.16,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional#code,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional/review/?hl=1747,2018,"Google, Google AI",,
BERT,85.083,91.835,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional#code,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional/review/?hl=1748,2018,"Google, Google AI",,
Reinforced Mnemonic Reader,82.283,88.533,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine#code,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine/review/?hl=1763,2017,Microsoft Asia,"National University of Defense Technology, Fudan University",
BiDAF + Self Attention + ELMo,81.003,87.432,https://paperswithcode.com/paper/deep-contextualized-word-representations,Deep contextualized word representations,https://paperswithcode.com/paper/deep-contextualized-word-representations#code,https://paperswithcode.com/paper/deep-contextualized-word-representations/review/?hl=2877,2018,AI2,University of Washington,
MAMCN+,79.692,86.727,https://paperswithcode.com/paper/a-multi-stage-memory-augmented-neural-network,A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension,,https://paperswithcode.com/paper/a-multi-stage-memory-augmented-neural-network#results,2018,Samsung,,
SAN,79.608,86.496,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine,Stochastic Answer Networks for Machine Reading Comprehension,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine#code,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine/review/?hl=1790,2017,Microsoft,Johns Hopkins University,
Reinforced Mnemonic Reader,79.545,86.654,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine#code,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine/review/?hl=1785,2017,Microsoft Asia,"National University of Defense Technology, Fudan University",
FusionNet,78.978,86.016,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention#code,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention/review/?hl=1795,2017,Microsoft,National Taiwan University,
DCN+,78.852,85.996,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual#code,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual/review/?hl=1796,2017,Salesforce,,
BiDAF + Self Attention + ELMo,78.58,85.833,https://paperswithcode.com/paper/deep-contextualized-word-representations,Deep contextualized word representations,https://paperswithcode.com/paper/deep-contextualized-word-representations#code,https://paperswithcode.com/paper/deep-contextualized-word-representations/review/?hl=2876,2018,AI2,University of Washington,
MEMEN,78.234,85.344,https://paperswithcode.com/paper/memen-multi-layer-embedding-with-memory,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,,https://paperswithcode.com/paper/memen-multi-layer-embedding-with-memory#results,2017,Eigen Technologies,Zhejiang University,
MEMEN,78.234,85.344,https://paperswithcode.com/paper/memen-multi-layer-embedding-with-memory,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,,https://paperswithcode.com/paper/memen-multi-layer-embedding-with-memory#results,2017,Eigen Technologies,Zhejiang University,
RaSoR + TR + LM,77.583,84.163,https://paperswithcode.com/paper/contextualized-word-representations-for,Contextualized Word Representations for Reading Comprehension,https://paperswithcode.com/paper/contextualized-word-representations-for#code,https://paperswithcode.com/paper/contextualized-word-representations-for/review/?hl=1811,2017,,Tel-Aviv University,
Conductor-net,76.996,84.63,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions,Phase Conductor on Multi-layered Attentions for Machine Comprehension,,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions/review/?hl=1801,2017,,"Carnegie Mellon University, University of Pittsburgh",
SAN,76.828,84.396,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine,Stochastic Answer Networks for Machine Reading Comprehension,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine#code,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine/review/?hl=1818,2017,Microsoft,Johns Hopkins University,
r-net,76.461,84.265,https://paperswithcode.com/paper/gated-self-matching-networks-for-reading,Gated Self-Matching Networks for Reading Comprehension and Question Answering,,https://paperswithcode.com/paper/gated-self-matching-networks-for-reading#results,2017,Microsoft Asia,Peking University,
QANet + data augmentation Ã—3,76.2,84.6,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global#code,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global/review/?hl=2960,2018,Google Brain,Carnegie Mellon University,
KAR,76.125,83.538,https://paperswithcode.com/paper/exploring-machine-reading-comprehension-with,Explicit Utilization of General Knowledge in Machine Reading Comprehension,,https://paperswithcode.com/paper/exploring-machine-reading-comprehension-with/review/?hl=1824,2018,,York University,
FusionNet,75.968,83.9,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention#code,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention/review/?hl=1823,2017,Microsoft,National Taiwan University,
RaSoR + TR,75.789,83.261,https://paperswithcode.com/paper/contextualized-word-representations-for,Contextualized Word Representations for Reading Comprehension,https://paperswithcode.com/paper/contextualized-word-representations-for#code,https://paperswithcode.com/paper/contextualized-word-representations-for/review/?hl=1829,2017,,Tel-Aviv University,
MEMEN,75.37,82.658,https://paperswithcode.com/paper/memen-multi-layer-embedding-with-memory,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,,https://paperswithcode.com/paper/memen-multi-layer-embedding-with-memory#results,2017,Eigen Technologies,Zhejiang University,
ReasoNet,75.034,82.552,https://paperswithcode.com/paper/reasonet-learning-to-stop-reading-in-machine,ReasoNet: Learning to Stop Reading in Machine Comprehension,,https://paperswithcode.com/paper/reasonet-learning-to-stop-reading-in-machine/review/?hl=1836,2016,Microsoft,,
DCN+,74.866,82.806,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual#code,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual/review/?hl=1830,2017,Salesforce,,
Conductor-net,74.405,82.742,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions,Phase Conductor on Multi-layered Attentions for Machine Comprehension,,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions/review/?hl=1833,2017,,"Carnegie Mellon University, University of Pittsburgh",
Mnemonic Reader,74.268,82.371,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine#code,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine/review/?hl=1842,2017,Microsoft Asia,"National University of Defense Technology, Fudan University",
SEDT,74.09,81.761,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for,Structural Embedding of Syntactic Trees for Machine Comprehension,,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for/review/?hl=1844,2017,,Carnegie Mellon University,
Multi-Perspective Matching,73.765,81.257,https://paperswithcode.com/paper/multi-perspective-context-matching-for,Multi-Perspective Context Matching for Machine Comprehension,https://paperswithcode.com/paper/multi-perspective-context-matching-for#code,https://paperswithcode.com/paper/multi-perspective-context-matching-for/review/?hl=1850,2016,IBM,,
BiDAF,73.744,81.525,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine,Bidirectional Attention Flow for Machine Comprehension,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine#code,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine/review/?hl=1851,2016,AI2,University of Washington,
SEDT+BiDAF,73.723,81.53,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for,Structural Embedding of Syntactic Trees for Machine Comprehension,,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for/review/?hl=1849,2017,,Carnegie Mellon University,
Conductor-net,73.24,81.933,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions,Phase Conductor on Multi-layered Attentions for Machine Comprehension,,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions/review/?hl=1847,2017,,"Carnegie Mellon University, University of Pittsburgh",
jNet,73.01,81.517,https://paperswithcode.com/paper/exploring-question-understanding-and,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,,https://paperswithcode.com/paper/exploring-question-understanding-and/review/?hl=1852,2017,iFLYTEK,"York University, University of Science and Technology China, National Research Council Canada",
BiDAF + Self Attention,72.139,81.048,https://paperswithcode.com/paper/simple-and-effective-multi-paragraph-reading,Simple and Effective Multi-Paragraph Reading Comprehension,https://paperswithcode.com/paper/simple-and-effective-multi-paragraph-reading#code,https://paperswithcode.com/paper/simple-and-effective-multi-paragraph-reading/review/?hl=1857,2017,AI2,University of Washington,
Dynamic Coattention Networks,71.625,80.383,https://paperswithcode.com/paper/dynamic-coattention-networks-for-question,Dynamic Coattention Networks For Question Answering,https://paperswithcode.com/paper/dynamic-coattention-networks-for-question#code,https://paperswithcode.com/paper/dynamic-coattention-networks-for-question/review/?hl=1861,2016,Salesforce,,
smarnet,71.415,80.16,https://paperswithcode.com/paper/smarnet-teaching-machines-to-read-and,Smarnet: Teaching Machines to Read and Comprehend Like Human,,https://paperswithcode.com/paper/smarnet-teaching-machines-to-read-and/review/?hl=1862,2017,Eigen Technologies,Zhejiang University,
SRU,71.4,80.2,https://paperswithcode.com/paper/simple-recurrent-units-for-highly,Simple Recurrent Units for Highly Parallelizable Recurrence,https://paperswithcode.com/paper/simple-recurrent-units-for-highly#code,https://paperswithcode.com/paper/simple-recurrent-units-for-highly/review/?hl=2905,2017,,,
DCN + Char + CoVe,71.3,79.9,https://paperswithcode.com/paper/learned-in-translation-contextualized-word,Learned in Translation: Contextualized Word Vectors,https://paperswithcode.com/paper/learned-in-translation-contextualized-word#code,https://paperswithcode.com/paper/learned-in-translation-contextualized-word/review/?hl=2923,2017,"ASAPP, Google Brain","Princeton University, Cornell University",
Mnemonic Reader,70.995,80.146,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine#code,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine/review/?hl=1863,2017,Microsoft Asia,"National University of Defense Technology, Fudan University",
FastQAExt,70.849,78.857,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but,Making Neural QA as Simple as Possible but not Simpler,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but#code,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but/review/?hl=1872,2017,DFKI,,
RaSoR,70.849,78.741,https://paperswithcode.com/paper/learning-recurrent-span-representations-for,Learning Recurrent Span Representations for Extractive Question Answering,https://paperswithcode.com/paper/learning-recurrent-span-representations-for#code,https://paperswithcode.com/paper/learning-recurrent-span-representations-for/review/?hl=1870,2016,Google,"University of Washington, Tel-Aviv University",
Document Reader,70.733,79.353,https://paperswithcode.com/paper/reading-wikipedia-to-answer-open-domain,Reading Wikipedia to Answer Open-Domain Questions,https://paperswithcode.com/paper/reading-wikipedia-to-answer-open-domain#code,https://paperswithcode.com/paper/reading-wikipedia-to-answer-open-domain/review/?hl=1871,2017,Facebook AI Research,Stanford University,
Ruminating Reader,70.639,79.456,https://paperswithcode.com/paper/ruminating-reader-reasoning-with-gated-multi,Ruminating Reader: Reasoning with Gated Multi-Hop Attention,,https://paperswithcode.com/paper/ruminating-reader-reasoning-with-gated-multi/review/?hl=1869,2017,,New York University,
jNet,70.607,79.821,https://paperswithcode.com/paper/exploring-question-understanding-and,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,,https://paperswithcode.com/paper/exploring-question-understanding-and/review/?hl=1867,2017,iFLYTEK,"York University, University of Science and Technology China, National Research Council Canada",
ReasoNet,70.555,79.364,https://paperswithcode.com/paper/reasonet-learning-to-stop-reading-in-machine,ReasoNet: Learning to Stop Reading in Machine Comprehension,,https://paperswithcode.com/paper/reasonet-learning-to-stop-reading-in-machine/review/?hl=1873,2016,Microsoft,,
Multi-Perspective Matching,70.387,78.784,https://paperswithcode.com/paper/multi-perspective-context-matching-for,Multi-Perspective Context Matching for Machine Comprehension,https://paperswithcode.com/paper/multi-perspective-context-matching-for#code,https://paperswithcode.com/paper/multi-perspective-context-matching-for/review/?hl=1874,2016,IBM,,
SEDT+BiDAF,68.478,77.971,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for,Structural Embedding of Syntactic Trees for Machine Comprehension,,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for/review/?hl=1877,2017,,Carnegie Mellon University,
FastQA,68.436,77.07,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but,Making Neural QA as Simple as Possible but not Simpler,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but#code,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but/review/?hl=1883,2017,DFKI,,
SEDT,68.163,77.527,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for,Structural Embedding of Syntactic Trees for Machine Comprehension,,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for/review/?hl=1880,2017,,Carnegie Mellon University,
BiDAF,67.974,77.323,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine,Bidirectional Attention Flow for Machine Comprehension,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine#code,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine/review/?hl=1881,2016,AI2,University of Washington,
Match-LSTM with Ans-Ptr,67.901,77.022,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and,Machine Comprehension Using Match-LSTM and Answer Pointer,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and#code,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and/review/?hl=1887,2016,,Singapore Management University,
FABIR,67.744,77.605,https://paperswithcode.com/paper/a-fully-attention-based-information-retriever,A Fully Attention-Based Information Retriever,https://paperswithcode.com/paper/a-fully-attention-based-information-retriever#code,https://paperswithcode.com/paper/a-fully-attention-based-information-retriever#results,2018,,Escola Politecnica-Universidadede Sao Paulo,
Match-LSTM with Bi-Ans-Ptr,64.744,73.743,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and,Machine Comprehension Using Match-LSTM and Answer Pointer,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and#code,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and/review/?hl=1894,2016,,Singapore Management University,
OTF dict+spelling,64.083,73.056,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the,Learning to Compute Word Embeddings On the Fly,,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the/review/?hl=1896,2017,DeepMind,University of Montreal,
OTF spelling,62.897,72.016,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the,Learning to Compute Word Embeddings On the Fly,,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the/review/?hl=1897,2017,DeepMind,University of Montreal,
OTF spelling+lemma,62.604,71.968,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the,Learning to Compute Word Embeddings On the Fly,,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the/review/?hl=1899,2017,DeepMind,University of Montreal,
Fine-Grained Gating,62.446,73.327,https://paperswithcode.com/paper/words-or-characters-fine-grained-gating-for,Words or Characters? Fine-grained Gating for Reading Comprehension,https://paperswithcode.com/paper/words-or-characters-fine-grained-gating-for#code,https://paperswithcode.com/paper/words-or-characters-fine-grained-gating-for/review/?hl=1898,2016,,Carnegie Mellon University,
RQA+IDR,61.145,71.389,https://paperswithcode.com/paper/harvesting-and-refining-question-answer-pairs,Harvesting and Refining Question-Answer Pairs for Unsupervised QA,https://paperswithcode.com/paper/harvesting-and-refining-question-answer-pairs#code,https://paperswithcode.com/paper/harvesting-and-refining-question-answer-pairs#results,2020,Microsoft,Beihang University,
Match-LSTM with Ans-Ptr,60.474,70.695,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and,Machine Comprehension Using Match-LSTM and Answer Pointer,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and#code,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and/review/?hl=1901,2016,,Singapore Management University,
RQA,55.827,65.467,https://paperswithcode.com/paper/harvesting-and-refining-question-answer-pairs,Harvesting and Refining Question-Answer Pairs for Unsupervised QA,https://paperswithcode.com/paper/harvesting-and-refining-question-answer-pairs#code,https://paperswithcode.com/paper/harvesting-and-refining-question-answer-pairs#results,2020,Microsoft,Beihang University,
Match-LSTM with Ans-Ptr,54.505,67.748,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and,Machine Comprehension Using Match-LSTM and Answer Pointer,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and#code,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and/review/?hl=1904,2016,,Singapore Management University,
BERT-Large 32k batch size with AdamW,91.58,,https://paperswithcode.com/paper/a-large-batch-optimizer-reality-check,"A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes",,https://paperswithcode.com/paper/a-large-batch-optimizer-reality-check/review/?hl=25984,2021,Google Brain,Harvard University,
RuBERT,84.6,,https://paperswithcode.com/paper/adaptation-of-deep-bidirectional-multilingual,Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language,https://paperswithcode.com/paper/adaptation-of-deep-bidirectional-multilingual#code,https://paperswithcode.com/paper/adaptation-of-deep-bidirectional-multilingual/review/?hl=16542,2019,,Moscow Institute of Physics and Technology,