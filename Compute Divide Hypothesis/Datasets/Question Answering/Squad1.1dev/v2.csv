Model,Paper Link,EM Score,F1 Score,Title,github href,paper href,Year,Industry Affiliation,Academia Affiliation,Country
T5-11B,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,90.06,95.64,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8370,2019,Google,,
LUKE,https://paperswithcode.com/paper/luke-deep-contextualized-entity,89.8,95,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://paperswithcode.com/paper/luke-deep-contextualized-entity#code,https://paperswithcode.com/paper/luke-deep-contextualized-entity/review/?hl=20506,2020,Studio Ousia,"RIKEN, University of Washington, NAIST, National Institute of Informatics",
XLNet+DSC,https://paperswithcode.com/paper/dice-loss-for-data-imbalanced-nlp-tasks,89.79,95.77,Dice Loss for Data-imbalanced NLP Tasks,https://paperswithcode.com/paper/dice-loss-for-data-imbalanced-nlp-tasks#code,https://paperswithcode.com/paper/dice-loss-for-data-imbalanced-nlp-tasks/review/?hl=10638,2019,Shannon AI,Zhejiang University,
XLNet,https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining,89.7,95.1,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining#code,https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining/review/?hl=13075,2019,Google Brain,Carnegie Mellon University,
T5-3B,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,88.53,94.95,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8371,2019,Google,,
T5-Large,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,86.66,93.79,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8372,2019,Google,,
T5-Base,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,85.44,92.08,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8373,2019,Google,,
BERT large,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional,84.2,91.1,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional#code,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional/review/?hl=8061,2018,Google AI,,
BERT large,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional,84.1,90.9,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional#code,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional/review/?hl=8060,2018,Google AI,,
BERT base,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional,80.8,88.5,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional#code,https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional/review/?hl=8059,2018,Google AI,,
TinyBERT,https://paperswithcode.com/paper/190910351,79.7,87.5,TinyBERT: Distilling BERT for Natural Language Understanding,https://paperswithcode.com/paper/190910351#code,https://paperswithcode.com/paper/190910351/review/?hl=9352,2019,Huawei Inc.,Huazhong University of Science and Technology,
T5-Small,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,79.1,87.24,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8374,2019,Google,,
R.M-Reader,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine,78.9,86.3,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine#code,https://paperswithcode.com/paper/reinforced-mnemonic-reader-for-machine/review/?hl=8062,2018,Microsoft Asia,"National University of Defense Technology, Fudan University",
DensePhrases,https://paperswithcode.com/paper/learning-dense-representations-of-phrases-at,78.3,86.3,Learning Dense Representations of Phrases at Scale,https://paperswithcode.com/paper/learning-dense-representations-of-phrases-at#code,https://paperswithcode.com/paper/learning-dense-representations-of-phrases-at/review/?hl=28110,2020,,"Korea University, Princeton University",
DistilBERT,https://paperswithcode.com/paper/distilbert-a-distilled-version-of-bert,77.7,85.8,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",https://paperswithcode.com/paper/distilbert-a-distilled-version-of-bert#code,https://paperswithcode.com/paper/distilbert-a-distilled-version-of-bert/review/?hl=8120,2019,Hugging Face,,
KAR,https://paperswithcode.com/paper/exploring-machine-reading-comprehension-with,76.7,84.9,Explicit Utilization of General Knowledge in Machine Reading Comprehension,,https://paperswithcode.com/paper/exploring-machine-reading-comprehension-with/review/?hl=8072,2018,,York University,
SAN,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine,76.235,84.056,Stochastic Answer Networks for Machine Reading Comprehension,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine#code,https://paperswithcode.com/paper/stochastic-answer-networks-for-machine/review/?hl=8064,2017,Microsoft,Johns Hopkins University,
FusionNet,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention,75.3,83.6,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention#code,https://paperswithcode.com/paper/fusionnet-fusing-via-fully-aware-attention/review/?hl=8065,2017,Microsoft,National Taiwan University,
QANet,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global,75.1,83.8,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global#code,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global/review/?hl=8069,2018,Google Brain,Carnegie Mellon University,
QANet,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global,74.5,83.2,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global#code,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global/review/?hl=8070,2018,Google Brain,Carnegie Mellon University,
DCN+,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual,74.5,83.1,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual#code,https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual/review/?hl=8066,2017,Salesforce,,
QANet,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global,73.6,82.7,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global#code,https://paperswithcode.com/paper/qanet-combining-local-convolution-with-global/review/?hl=8071,2018,Google Brain,Carnegie Mellon University,
PhaseCond,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions,72.1,81.4,Phase Conductor on Multi-layered Attentions for Machine Comprehension,,https://paperswithcode.com/paper/phase-conductor-on-multi-layered-attentions/review/?hl=8067,2017,,"Carnegie Mellon University, University of Pittsburgh",
SRU,https://paperswithcode.com/paper/simple-recurrent-units-for-highly,71.4,80.2,Simple Recurrent Units for Highly Parallelizable Recurrence,https://paperswithcode.com/paper/simple-recurrent-units-for-highly#code,https://paperswithcode.com/paper/simple-recurrent-units-for-highly/review/?hl=8080,2017,"ASAPP, Google Brain","Princeton University, Cornell University",
Smarnet,https://paperswithcode.com/paper/smarnet-teaching-machines-to-read-and,71.362,80.183,Smarnet: Teaching Machines to Read and Comprehend Like Human,,https://paperswithcode.com/paper/smarnet-teaching-machines-to-read-and/review/?hl=8079,2017,Eigen Technologies,Zhejiang University,
DCN,https://paperswithcode.com/paper/learned-in-translation-contextualized-word,71.3,79.9,Learned in Translation: Contextualized Word Vectors,https://paperswithcode.com/paper/learned-in-translation-contextualized-word#code,https://paperswithcode.com/paper/learned-in-translation-contextualized-word/review/?hl=8081,2017,Salesforce,,
R-NET,https://paperswithcode.com/paper/gated-self-matching-networks-for-reading,71.1,79.5,Gated Self-Matching Networks for Reading Comprehension and Question Answering,,https://paperswithcode.com/paper/gated-self-matching-networks-for-reading#results,2017,Microsoft Asia,Peking University,
Ruminating Reader,https://paperswithcode.com/paper/ruminating-reader-reasoning-with-gated-multi,70.6,79.5,Ruminating Reader: Reasoning with Gated Multi-Hop Attention,,https://paperswithcode.com/paper/ruminating-reader-reasoning-with-gated-multi/review/?hl=8085,2017,,New York University,
FastQAExt,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but,70.3,78.5,Making Neural QA as Simple as Possible but not Simpler,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but#code,https://paperswithcode.com/paper/making-neural-qa-as-simple-as-possible-but/review/?hl=8082,2017,DFKI,,
DrQA,https://paperswithcode.com/paper/reading-wikipedia-to-answer-open-domain,69.5,78.8,Reading Wikipedia to Answer Open-Domain Questions,https://paperswithcode.com/paper/reading-wikipedia-to-answer-open-domain#code,https://paperswithcode.com/paper/reading-wikipedia-to-answer-open-domain/review/?hl=8084,2017,Facebook AI Research,Stanford University,
jNet,https://paperswithcode.com/paper/exploring-question-understanding-and,69.1,78.38,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,,https://paperswithcode.com/paper/exploring-question-understanding-and/review/?hl=8077,2017,iFLYTEK,"York University, University of Science and Technology China, National Research Council Canada",
SEDT-LSTM,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for,67.89,77.42,Structural Embedding of Syntactic Trees for Machine Comprehension,,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for/review/?hl=8073,2017,,Carnegie Mellon University,
BIDAF,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine,67.7,77.3,Bidirectional Attention Flow for Machine Comprehension,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine#code,https://paperswithcode.com/paper/bidirectional-attention-flow-for-machine/review/?hl=8076,2016,AI2,University of Washington,
SECT-LSTM,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for,67.65,77.19,Structural Embedding of Syntactic Trees for Machine Comprehension,,https://paperswithcode.com/paper/structural-embedding-of-syntactic-trees-for/review/?hl=8074,2017,,Carnegie Mellon University,
RASOR,https://paperswithcode.com/paper/learning-recurrent-span-representations-for,66.4,74.9,Learning Recurrent Span Representations for Extractive Question Answering,https://paperswithcode.com/paper/learning-recurrent-span-representations-for#code,https://paperswithcode.com/paper/learning-recurrent-span-representations-for/review/?hl=8083,2016,Google,"University of Washington, Tel-Aviv University",
MPCM,https://paperswithcode.com/paper/multi-perspective-context-matching-for,66.1,75.8,Multi-Perspective Context Matching for Machine Comprehension,https://paperswithcode.com/paper/multi-perspective-context-matching-for#code,https://paperswithcode.com/paper/multi-perspective-context-matching-for/review/?hl=8075,2016,IBM,,
DCN,https://paperswithcode.com/paper/dynamic-coattention-networks-for-question,65.4,75.6,Dynamic Coattention Networks For Question Answering,https://paperswithcode.com/paper/dynamic-coattention-networks-for-question#code,https://paperswithcode.com/paper/dynamic-coattention-networks-for-question/review/?hl=8078,2016,Salesforce,,
FABIR,https://paperswithcode.com/paper/a-fully-attention-based-information-retriever,65.1,75.6,A Fully Attention-Based Information Retriever,https://paperswithcode.com/paper/a-fully-attention-based-information-retriever#code,https://paperswithcode.com/paper/a-fully-attention-based-information-retriever#results,2018,,Escola Politecnica-Universidadede Sao Paulo,
Match-LSTM with Bi-Ans-Ptr,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and,64.1,64.7,Machine Comprehension Using Match-LSTM and Answer Pointer,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and#code,https://paperswithcode.com/paper/machine-comprehension-using-match-lstm-and/review/?hl=8088,2016,,Singapore Management University,
OTF dict+spelling,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the,63.06,,Learning to Compute Word Embeddings On the Fly,,https://paperswithcode.com/paper/learning-to-compute-word-embeddings-on-the/review/?hl=8090,2017,DeepMind,University of Montreal,
DCR,https://paperswithcode.com/paper/end-to-end-answer-chunk-extraction-and,62.5,71.2,End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,,https://paperswithcode.com/paper/end-to-end-answer-chunk-extraction-and/review/?hl=8091,2016,IBM,,
FG fine-grained gate,https://paperswithcode.com/paper/words-or-characters-fine-grained-gating-for,59.95,71.25,Words or Characters? Fine-grained Gating for Reading Comprehension,https://paperswithcode.com/paper/words-or-characters-fine-grained-gating-for#code,https://paperswithcode.com/paper/words-or-characters-fine-grained-gating-for/review/?hl=8092,2016,,Carnegie Mellon University,
BART Base,https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre,90.8,,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre#code,https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre/review/?hl=13076,2019,Facebook AI Research,,
BERT large,https://paperswithcode.com/paper/reducing-bert-pre-training-time-from-3-days,90.584,,Large Batch Optimization for Deep Learning: Training BERT in 76 minutes,https://paperswithcode.com/paper/reducing-bert-pre-training-time-from-3-days#code,https://paperswithcode.com/paper/reducing-bert-pre-training-time-from-3-days/review/?hl=8313,2019,Google,"UCLA, UC Berkeley",
BiDAF + Self Attention + ELMo,https://paperswithcode.com/paper/deep-contextualized-word-representations,85.6,,Deep contextualized word representations,https://paperswithcode.com/paper/deep-contextualized-word-representations#code,https://paperswithcode.com/paper/deep-contextualized-word-representations/review/?hl=8063,2018,AI2,University of Washington,