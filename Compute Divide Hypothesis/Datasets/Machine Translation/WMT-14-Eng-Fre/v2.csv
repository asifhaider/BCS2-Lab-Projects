Model,Paper Link,BLEU Score,Extra Training Data,github href,paper href,Title,Year,Industry Affiliation,Academia Affiliation,Country
Transformer+BT,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine,46.4,Yes,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine#code,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine/review/?hl=21220,Very Deep Transformers for Neural Machine Translation,2020,Microsoft,"Johns Hopkins University, UIUC",
Noisy back-translation,https://paperswithcode.com/paper/understanding-back-translation-at-scale,45.6,Yes,https://paperswithcode.com/paper/understanding-back-translation-at-scale#code,https://paperswithcode.com/paper/understanding-back-translation-at-scale/review/?hl=117,Understanding Back-Translation at Scale,2018,"Facebook AI Research, Google Brain",,
mRASP+Fine-Tune,https://paperswithcode.com/paper/pre-training-multilingual-neural-machine,44.3,Yes,https://paperswithcode.com/paper/pre-training-multilingual-neural-machine#code,https://paperswithcode.com/paper/pre-training-multilingual-neural-machine/review/?hl=20788,Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information,2020,ByteDance,Fudan University,
Transformer + R-Drop,https://paperswithcode.com/paper/r-drop-regularized-dropout-for-neural,43.95,No,https://paperswithcode.com/paper/r-drop-regularized-dropout-for-neural#code,https://paperswithcode.com/paper/r-drop-regularized-dropout-for-neural/review/?hl=36287,R-Drop: Regularized Dropout for Neural Networks,2021,Microsoft Asia,Soochow University,
Transformer,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine,43.8,No,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine#code,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine/review/?hl=19190,Very Deep Transformers for Neural Machine Translation,2020,Microsoft,"Johns Hopkins University, UIUC",
Admin,https://paperswithcode.com/paper/understanding-the-difficulty-of-training,43.8,No,https://paperswithcode.com/paper/understanding-the-difficulty-of-training#code,https://paperswithcode.com/paper/understanding-the-difficulty-of-training/review/?hl=26105,Understanding the Difficulty of Training Transformers,2020,Microsoft,UIUC,
BERT-fused NMT,https://paperswithcode.com/paper/incorporating-bert-into-neural-machine-1,43.78,No,https://paperswithcode.com/paper/incorporating-bert-into-neural-machine-1#code,https://paperswithcode.com/paper/incorporating-bert-into-neural-machine-1/review/?hl=33216,Incorporating BERT into Neural Machine Translation,2020,Microsoft Asia,"University of Science and Technology China, SunYat-sen University, Peking University",
MUSE,https://paperswithcode.com/paper/muse-parallel-multi-scale-attention-for,43.5,No,https://paperswithcode.com/paper/muse-parallel-multi-scale-attention-for#code,https://paperswithcode.com/paper/muse-parallel-multi-scale-attention-for/review/?hl=8787,MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning,2019,,Peking University,
T5,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,43.4,Yes,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8789,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019,Google,,
Local Joint Self-attention,https://paperswithcode.com/paper/190506596,43.3,No,https://paperswithcode.com/paper/190506596#code,https://paperswithcode.com/paper/190506596/review/?hl=5700,Joint Source-Target Self Attention with Locality Constraints,2019,,Universitat Politecnica de Catalunya,
Depth Growing,https://paperswithcode.com/paper/depth-growing-for-neural-machine-translation,43.27,No,https://paperswithcode.com/paper/depth-growing-for-neural-machine-translation#code,https://paperswithcode.com/paper/depth-growing-for-neural-machine-translation/review/?hl=5864,Depth Growing for Neural Machine Translation,2019,Microsoft Asia,"UIUC, SunYat-sen University",
Transformer Big,https://paperswithcode.com/paper/scaling-neural-machine-translation,43.2,No,https://paperswithcode.com/paper/scaling-neural-machine-translation#code,https://paperswithcode.com/paper/scaling-neural-machine-translation/review/?hl=118,Scaling Neural Machine Translation,2018,"Facebook AI Research, Google Brain",,
TaLK Convolutions,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions,43.2,No,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions#code,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions/review/?hl=9913,Time-aware Large Kernel Convolutions,2020,,Carleton University,
LightConv,https://paperswithcode.com/paper/pay-less-attention-with-lightweight-and,43.1,No,https://paperswithcode.com/paper/pay-less-attention-with-lightweight-and#code,https://paperswithcode.com/paper/pay-less-attention-with-lightweight-and/review/?hl=3485,Pay Less Attention with Lightweight and Dynamic Convolutions,2019,Facebook AI Research,Cornell University,
FLOATER-large,https://paperswithcode.com/paper/learning-to-encode-position-for-transformer,42.7,No,https://paperswithcode.com/paper/learning-to-encode-position-for-transformer#code,https://paperswithcode.com/paper/learning-to-encode-position-for-transformer/review/?hl=26104,Learning to Encode Position for Transformer with Continuous Dynamical Model,2020,Amazon,"UT Austin, UCLA",
OmniNetP,https://paperswithcode.com/paper/omninet-omnidirectional-representations-from,42.6,No,https://paperswithcode.com/paper/omninet-omnidirectional-representations-from#code,https://paperswithcode.com/paper/omninet-omnidirectional-representations-from/review/?hl=27562,OmniNet: Omnidirectional Representations from Transformers,2021,"Google, Google Brain",,Netherlands
Transformer Big + MoS,https://paperswithcode.com/paper/fast-and-simple-mixture-of-softmaxes-with-bpe,42.1,No,https://paperswithcode.com/paper/fast-and-simple-mixture-of-softmaxes-with-bpe#code,https://paperswithcode.com/paper/fast-and-simple-mixture-of-softmaxes-with-bpe/review/?hl=7256,Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation,2018,,Carnegie Mellon University,
T2R + Pretrain,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns,42.1,No,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns#code,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns/review/?hl=28722,Finetuning Pretrained Transformers into RNNs,2021,"Microsoft, DeepMind, AI2",University of Washington,
Synthesizer,https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in,41.85,No,https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in#code,https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in/review/?hl=16665,Synthesizer: Rethinking Self-Attention in Transformer Models,2020,Google,,
Transformer,https://paperswithcode.com/paper/self-attention-with-relative-position,41.5,No,https://paperswithcode.com/paper/self-attention-with-relative-position#code,https://paperswithcode.com/paper/self-attention-with-relative-position/review/?hl=2780,Self-Attention with Relative Position Representations,2018,Google Brain,,
Weighted Transformer,https://paperswithcode.com/paper/weighted-transformer-network-for-machine,41.4,No,https://paperswithcode.com/paper/weighted-transformer-network-for-machine#code,https://paperswithcode.com/paper/weighted-transformer-network-for-machine/review/?hl=2855,Weighted Transformer Network for Machine Translation,2017,Salesforce,,
ConvS2S,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning,41.3,No,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#code,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#results,Convolutional Sequence to Sequence Learning,2017,Facebook AI Research,,
Evolved Transformer Big,https://paperswithcode.com/paper/the-evolved-transformer,41.3,No,https://paperswithcode.com/paper/the-evolved-transformer#code,https://paperswithcode.com/paper/the-evolved-transformer/review/?hl=3602,The Evolved Transformer,2019,Google Brain,,
RNMT+,https://paperswithcode.com/paper/the-best-of-both-worlds-combining-recent,41,No,https://paperswithcode.com/paper/the-best-of-both-worlds-combining-recent#code,https://paperswithcode.com/paper/the-best-of-both-worlds-combining-recent/review/?hl=119,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,2018,Google AI,,
Transformer Big,https://paperswithcode.com/paper/attention-is-all-you-need,41,No,https://paperswithcode.com/paper/attention-is-all-you-need#code,https://paperswithcode.com/paper/attention-is-all-you-need/review/?hl=120,Attention Is All You Need,2017,"Google, Google Brain",University of Toronto,
Evolved Transformer Base,https://paperswithcode.com/paper/the-evolved-transformer,40.6,No,https://paperswithcode.com/paper/the-evolved-transformer#code,https://paperswithcode.com/paper/the-evolved-transformer/review/?hl=3601,The Evolved Transformer,2019,Google Brain,,
ResMLP-12,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,40.6,No,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34916,ResMLP: Feedforward networks for image classification with data-efficient training,2021,"Facebook AI Research, Valeo","Sorbonne University, Inria",
MoE,https://paperswithcode.com/paper/outrageously-large-neural-networks-the,40.56,No,https://paperswithcode.com/paper/outrageously-large-neural-networks-the#code,https://paperswithcode.com/paper/outrageously-large-neural-networks-the/review/?hl=121,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,2017,Google Brain,Jagiellonian University,
Transformer,https://paperswithcode.com/paper/memory-efficient-adaptive-optimization-for,40.5,No,https://paperswithcode.com/paper/memory-efficient-adaptive-optimization-for#code,https://paperswithcode.com/paper/memory-efficient-adaptive-optimization-for/review/?hl=42537,Memory-Efficient Adaptive Optimization,2019,Google Brain,Princeton University,
ConvS2S,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning,40.46,No,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#code,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#results,Convolutional Sequence to Sequence Learning,2017,Facebook AI Research,,
ResMLP-6,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,40.3,No,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34915,ResMLP: Feedforward networks for image classification with data-efficient training,2021,"Facebook AI Research, Valeo","Sorbonne University, Inria",
TransformerBase + AutoDropout,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to,40,No,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to#code,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to/review/?hl=24601,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,2021,Google Brain,Carnegie Mellon University,
GNMT+RL,https://paperswithcode.com/paper/googles-neural-machine-translation-system,39.9,No,https://paperswithcode.com/paper/googles-neural-machine-translation-system#code,https://paperswithcode.com/paper/googles-neural-machine-translation-system/review/?hl=379,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,2016,Google,,
Deep-Att + PosUnk,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward,39.2,No,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward#code,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward/review/?hl=378,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,2016,Baidu Inc.,,
Rfa-Gate-arccos,https://paperswithcode.com/paper/random-feature-attention-1,39.2,No,,https://paperswithcode.com/paper/random-feature-attention-1/review/?hl=27544,Random Feature Attention,2021,"DeepMind, AI2","University of Washington, Hebrew University of Jerusalem, The University of Hong Kong",
Transformer Base,https://paperswithcode.com/paper/attention-is-all-you-need,38.1,No,https://paperswithcode.com/paper/attention-is-all-you-need#code,https://paperswithcode.com/paper/attention-is-all-you-need/review/?hl=123,Attention Is All You Need,2017,"Google, Google Brain",University of Toronto,
LSTM6 + PosUnk,https://paperswithcode.com/paper/addressing-the-rare-word-problem-in-neural,37.5,No,https://paperswithcode.com/paper/addressing-the-rare-word-problem-in-neural#code,https://paperswithcode.com/paper/addressing-the-rare-word-problem-in-neural/review/?hl=377,Addressing the Rare Word Problem in Neural Machine Translation,2014,Google,"New York University, Stanford University",
SMT+LSTM5,https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural,36.5,No,https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural#code,https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural/review/?hl=375,Sequence to Sequence Learning with Neural Networks,2014,Google,,
RNN-search50*,https://paperswithcode.com/paper/neural-machine-translation-by-jointly,36.2,No,https://paperswithcode.com/paper/neural-machine-translation-by-jointly#code,https://paperswithcode.com/paper/neural-machine-translation-by-jointly/review/?hl=374,Neural Machine Translation by Jointly Learning to Align and Translate,2014,,"Jacobs University Bremen, University of Montreal",
Deep-Att,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward,35.9,No,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward#code,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward/review/?hl=2785,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,2016,Baidu Inc.,,
Deep Convolutional Encoder; single-layer decoder,https://paperswithcode.com/paper/a-convolutional-encoder-model-for-neural,35.7,No,https://paperswithcode.com/paper/a-convolutional-encoder-model-for-neural#code,https://paperswithcode.com/paper/a-convolutional-encoder-model-for-neural#results,A Convolutional Encoder Model for Neural Machine Translation,2016,Facebook AI Research,,
LSTM,https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural,34.8,No,https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural#code,https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural/review/?hl=376,Sequence to Sequence Learning with Neural Networks,2014,Google,,
CSLM + RNN + WP,https://paperswithcode.com/paper/learning-phrase-representations-using-rnn,34.54,No,https://paperswithcode.com/paper/learning-phrase-representations-using-rnn#code,https://paperswithcode.com/paper/learning-phrase-representations-using-rnn/review/?hl=2788,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,2014,,"Jacobs University Bremen, University of Montreal, University of Le Mans",
FLAN 137B zero-shot,https://paperswithcode.com/paper/finetuned-language-models-are-zero-shot,34,No,https://paperswithcode.com/paper/finetuned-language-models-are-zero-shot#code,https://paperswithcode.com/paper/finetuned-language-models-are-zero-shot/review/?hl=39043,Finetuned Language Models Are Zero-Shot Learners,2021,Google,,
Regularized LSTM,https://paperswithcode.com/paper/recurrent-neural-network-regularization,29.03,No,https://paperswithcode.com/paper/recurrent-neural-network-regularization#code,https://paperswithcode.com/paper/recurrent-neural-network-regularization/review/?hl=2809,Recurrent Neural Network Regularization,2014,Google Brain,New York University,
Unsupervised PBSMT,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine,28.11,No,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine#code,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine/review/?hl=2791,Phrase-Based & Neural Unsupervised Machine Translation,2018,Facebook AI Research,Sorbonne University,
PBSMT + NMT,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine,27.6,No,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine#code,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine/review/?hl=2789,Phrase-Based & Neural Unsupervised Machine Translation,2018,Facebook AI Research,Sorbonne University,
GRU+Attention,https://paperswithcode.com/paper/can-active-memory-replace-attention,26.4,No,https://paperswithcode.com/paper/can-active-memory-replace-attention#code,https://paperswithcode.com/paper/can-active-memory-replace-attention/review/?hl=2775,Can Active Memory Replace Attention?,2016,Google Brain,,
SMT + iterative backtranslation,https://paperswithcode.com/paper/unsupervised-statistical-machine-translation,26.22,No,https://paperswithcode.com/paper/unsupervised-statistical-machine-translation#code,https://paperswithcode.com/paper/unsupervised-statistical-machine-translation/review/?hl=2828,Unsupervised Statistical Machine Translation,2018,,University of the Basque Country,
Unsupervised NMT + Transformer,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine,25.14,No,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine#code,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine/review/?hl=2792,Phrase-Based & Neural Unsupervised Machine Translation,2018,Facebook AI Research,Sorbonne University,
Unsupervised attentional encoder-decoder + BPE,https://paperswithcode.com/paper/unsupervised-neural-machine-translation,14.36,No,https://paperswithcode.com/paper/unsupervised-neural-machine-translation#code,https://paperswithcode.com/paper/unsupervised-neural-machine-translation/review/?hl=2807,Unsupervised Neural Machine Translation,2017,,"University of the Basque Country, New York University",