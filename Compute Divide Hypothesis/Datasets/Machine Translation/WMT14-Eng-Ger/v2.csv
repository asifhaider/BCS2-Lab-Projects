Model,Paper Link,BLEU Score,Year,github href,paper href,Title,Extra Training Data,Industry Affiliation,Academia Affiliation,Country
Transformer Cycle,https://paperswithcode.com/paper/lessons-on-parameter-sharing-across-layers-in,35.14,2021,https://paperswithcode.com/paper/lessons-on-parameter-sharing-across-layers-in#code,https://paperswithcode.com/paper/lessons-on-parameter-sharing-across-layers-in/review/?hl=30062,Lessons on Parameter Sharing across Layers in Transformers,Yes,,"Tokyo Institute of Technology, RIKEN",
Noisy back-translation,https://paperswithcode.com/paper/understanding-back-translation-at-scale,35,2018,https://paperswithcode.com/paper/understanding-back-translation-at-scale#code,https://paperswithcode.com/paper/understanding-back-translation-at-scale/review/?hl=108,Understanding Back-Translation at Scale,Yes,"Facebook AI Research, Google Brain",,
Transformer+Rep,https://paperswithcode.com/paper/rethinking-perturbations-in-encoder-decoders,33.89,2021,https://paperswithcode.com/paper/rethinking-perturbations-in-encoder-decoders#code,https://paperswithcode.com/paper/rethinking-perturbations-in-encoder-decoders/review/?hl=29732,Rethinking Perturbations in Encoder-Decoders for Fast Training,Yes,,"Tokyo Institute of Technology, RIKEN",
T5-11B,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning,32.1,2019,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#code,https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning/review/?hl=8382,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,Yes,Google,,
BiBERT,https://paperswithcode.com/paper/bert-mbert-or-bibert-a-study-on,31.26,2021,https://paperswithcode.com/paper/bert-mbert-or-bibert-a-study-on#code,https://paperswithcode.com/paper/bert-mbert-or-bibert-a-study-on/review/?hl=39391,"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation",No,,Johns Hopkins University,
Transformer + R-Drop,https://paperswithcode.com/paper/r-drop-regularized-dropout-for-neural,30.91,2021,https://paperswithcode.com/paper/r-drop-regularized-dropout-for-neural#code,https://paperswithcode.com/paper/r-drop-regularized-dropout-for-neural/review/?hl=36286,R-Drop: Regularized Dropout for Neural Networks,No,Microsoft Asia,Soochow University,
BERT-fused NMT,https://paperswithcode.com/paper/incorporating-bert-into-neural-machine-1,30.75,2020,https://paperswithcode.com/paper/incorporating-bert-into-neural-machine-1#code,https://paperswithcode.com/paper/incorporating-bert-into-neural-machine-1/review/?hl=17479,Incorporating BERT into Neural Machine Translation,No,Microsoft Asia,"University of Science and Technology China, SunYat-sen University, Peking University",
Data Diversification - Transformer,https://paperswithcode.com/paper/data-diversification-an-elegant-strategy-for,30.7,2019,https://paperswithcode.com/paper/data-diversification-an-elegant-strategy-for#code,https://paperswithcode.com/paper/data-diversification-an-elegant-strategy-for/review/?hl=8973,Data Diversification: A Simple Strategy For Neural Machine Translation,No,Salesforce,"Nanyang Technological University, I2R",
Mask Attention Network,https://paperswithcode.com/paper/mask-attention-networks-rethinking-and,30.4,2021,,https://paperswithcode.com/paper/mask-attention-networks-rethinking-and/review/?hl=28793,Mask Attention Networks: Rethinking and Strengthen Transformer,No,"Microsoft Asia, Microsoft","Fudan University, Damo Academy",
Transformer,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine,30.1,2020,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine#code,https://paperswithcode.com/paper/very-deep-transformers-for-neural-machine/review/?hl=19191,Very Deep Transformers for Neural Machine Translation,No,Microsoft,"Johns Hopkins University, UIUC",
Depth Growing,https://paperswithcode.com/paper/depth-growing-for-neural-machine-translation,30.07,2019,https://paperswithcode.com/paper/depth-growing-for-neural-machine-translation#code,https://paperswithcode.com/paper/depth-growing-for-neural-machine-translation/review/?hl=5862,Depth Growing for Neural Machine Translation,No,Microsoft Asia,"UIUC, SunYat-sen University",
MUSE,https://paperswithcode.com/paper/muse-parallel-multi-scale-attention-for,29.9,2019,https://paperswithcode.com/paper/muse-parallel-multi-scale-attention-for#code,https://paperswithcode.com/paper/muse-parallel-multi-scale-attention-for/review/?hl=8786,MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning,No,,Peking University,
Evolved Transformer Big,https://paperswithcode.com/paper/the-evolved-transformer,29.8,2019,https://paperswithcode.com/paper/the-evolved-transformer#code,https://paperswithcode.com/paper/the-evolved-transformer/review/?hl=11176,The Evolved Transformer,No,Google Brain,,
OmniNetP,https://paperswithcode.com/paper/omninet-omnidirectional-representations-from,29.8,2021,https://paperswithcode.com/paper/omninet-omnidirectional-representations-from#code,https://paperswithcode.com/paper/omninet-omnidirectional-representations-from/review/?hl=27561,OmniNet: Omnidirectional Representations from Transformers,No,"Google, Google Brain",,Netherlands
DynamicConv,https://paperswithcode.com/paper/pay-less-attention-with-lightweight-and,29.7,2019,https://paperswithcode.com/paper/pay-less-attention-with-lightweight-and#code,https://paperswithcode.com/paper/pay-less-attention-with-lightweight-and/review/?hl=3482,Pay Less Attention with Lightweight and Dynamic Convolutions,No,Facebook AI Research,Cornell University,
Local Joint Self-attention,https://paperswithcode.com/paper/190506596,29.7,2019,https://paperswithcode.com/paper/190506596#code,https://paperswithcode.com/paper/190506596/review/?hl=5699,Joint Source-Target Self Attention with Locality Constraints,No,,Universitat Politecnica de Catalunya,
Transformer Big + MoS,https://paperswithcode.com/paper/fast-and-simple-mixture-of-softmaxes-with-bpe,29.6,2018,https://paperswithcode.com/paper/fast-and-simple-mixture-of-softmaxes-with-bpe#code,https://paperswithcode.com/paper/fast-and-simple-mixture-of-softmaxes-with-bpe/review/?hl=7255,Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation,No,,Carnegie Mellon University,
TaLK Convolutions,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions,29.6,2020,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions#code,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions/review/?hl=9914,Time-aware Large Kernel Convolutions,No,,Carleton University,
Transformer Big + adversarial MLE,https://paperswithcode.com/paper/improving-neural-language-modeling-via,29.52,2019,https://paperswithcode.com/paper/improving-neural-language-modeling-via#code,https://paperswithcode.com/paper/improving-neural-language-modeling-via/review/?hl=6164,Improving Neural Language Modeling via Adversarial Training,No,,UT Austin,
Transformer Big,https://paperswithcode.com/paper/scaling-neural-machine-translation,29.3,2018,https://paperswithcode.com/paper/scaling-neural-machine-translation#code,https://paperswithcode.com/paper/scaling-neural-machine-translation/review/?hl=110,Scaling Neural Machine Translation,No,"Facebook AI Research, Google Brain",,
Evolved Transformer Big,https://paperswithcode.com/paper/the-evolved-transformer,29.3,2019,https://paperswithcode.com/paper/the-evolved-transformer#code,https://paperswithcode.com/paper/the-evolved-transformer/review/?hl=3600,The Evolved Transformer,No,Google Brain,,
SB-NMT,https://paperswithcode.com/paper/synchronous-bidirectional-neural-machine,29.21,2019,https://paperswithcode.com/paper/synchronous-bidirectional-neural-machine#code,https://paperswithcode.com/paper/synchronous-bidirectional-neural-machine/review/?hl=6212,Synchronous Bidirectional Neural Machine Translation,No,,"University of Chinese Academy of Sciences, CAS Center for Excellence in Brain Science and Intelligence Technology, CASIA",
Transformer,https://paperswithcode.com/paper/self-attention-with-relative-position,29.2,2018,https://paperswithcode.com/paper/self-attention-with-relative-position#code,https://paperswithcode.com/paper/self-attention-with-relative-position/review/?hl=2779,Self-Attention with Relative Position Representations,No,"Google, Google Brain",,
FLOATER-large,https://paperswithcode.com/paper/learning-to-encode-position-for-transformer,29.2,2020,https://paperswithcode.com/paper/learning-to-encode-position-for-transformer#code,https://paperswithcode.com/paper/learning-to-encode-position-for-transformer/review/?hl=26103,Learning to Encode Position for Transformer with Continuous Dynamical Model,No,Amazon,"UT Austin, UCLA",
Local Transformer,https://paperswithcode.com/paper/modeling-localness-for-self-attention,29.2,2018,,https://paperswithcode.com/paper/modeling-localness-for-self-attention/review/?hl=28797,Modeling Localness for Self-Attention Networks,No,Tencent,University of Macau,
Transformer Big with FRAGE,https://paperswithcode.com/paper/frage-frequency-agnostic-word-representation,29.11,2018,https://paperswithcode.com/paper/frage-frequency-agnostic-word-representation#code,https://paperswithcode.com/paper/frage-frequency-agnostic-word-representation/review/?hl=2865,FRAGE: Frequency-Agnostic Word Representation,No,Microsoft Asia,Peking University,
Mask Attention Network,https://paperswithcode.com/paper/mask-attention-networks-rethinking-and,29.1,2021,,https://paperswithcode.com/paper/mask-attention-networks-rethinking-and/review/?hl=28792,Mask Attention Networks: Rethinking and Strengthen Transformer,No,"Microsoft Asia, Microsoft","Fudan University, Damo Academy",
adequacy-oriented NMT,https://paperswithcode.com/paper/neural-machine-translation-with-adequacy,28.99,2018,,https://paperswithcode.com/paper/neural-machine-translation-with-adequacy/review/?hl=7339,Neural Machine Translation with Adequacy-Oriented Learning,No,Tencent,Carnegie Mellon University,
Weighted Transformer,https://paperswithcode.com/paper/weighted-transformer-network-for-machine,28.9,2017,https://paperswithcode.com/paper/weighted-transformer-network-for-machine#code,https://paperswithcode.com/paper/weighted-transformer-network-for-machine/review/?hl=2854,Weighted Transformer Network for Machine Translation,No,Salesforce,,
universal transformer base,https://paperswithcode.com/paper/universal-transformers,28.9,2018,https://paperswithcode.com/paper/universal-transformers#code,https://paperswithcode.com/paper/universal-transformers/review/?hl=3654,Universal Transformers,No,"Google Brain, DeepMind",University of Amsterdam,
KERMIT,https://paperswithcode.com/paper/kermit-generative-insertion-based-modeling,28.7,2019,,https://paperswithcode.com/paper/kermit-generative-insertion-based-modeling/review/?hl=5616,KERMIT: Generative Insertion-Based Modeling for Sequences,No,"Google AI, Google Brain",UC Berkeley,
T2R + Pretrain,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns,28.7,2021,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns#code,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns/review/?hl=28721,Finetuning Pretrained Transformers into RNNs,No,"Microsoft, DeepMind, AI2",University of Washington,
RNMT+,https://paperswithcode.com/paper/the-best-of-both-worlds-combining-recent,28.5,2018,https://paperswithcode.com/paper/the-best-of-both-worlds-combining-recent#code,https://paperswithcode.com/paper/the-best-of-both-worlds-combining-recent/review/?hl=111,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,No,Google AI,,
Synthesizer,https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in,28.47,2020,https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in#code,https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in/review/?hl=16664,Synthesizer: Rethinking Self-Attention in Transformer Models,No,Google,,
Transformer Big,https://paperswithcode.com/paper/attention-is-all-you-need,28.4,2017,https://paperswithcode.com/paper/attention-is-all-you-need#code,https://paperswithcode.com/paper/attention-is-all-you-need/review/?hl=112,Attention Is All You Need,No,"Google, Google Brain",University of Toronto,
Transformer + SRU,https://paperswithcode.com/paper/simple-recurrent-units-for-highly,28.4,2017,https://paperswithcode.com/paper/simple-recurrent-units-for-highly#code,https://paperswithcode.com/paper/simple-recurrent-units-for-highly/review/?hl=2906,Simple Recurrent Units for Highly Parallelizable Recurrence,No,ASAPP,"Princeton University, Cornell University",
Evolved Transformer Base,https://paperswithcode.com/paper/the-evolved-transformer,28.4,2019,https://paperswithcode.com/paper/the-evolved-transformer#code,https://paperswithcode.com/paper/the-evolved-transformer/review/?hl=3599,The Evolved Transformer,No,Google Brain,,
Rfa-Gate-arccos,https://paperswithcode.com/paper/random-feature-attention-1,28.2,2021,,https://paperswithcode.com/paper/random-feature-attention-1/review/?hl=27543,Random Feature Attention,No,"DeepMind, AI2","University of Washington, Hebrew University of Jerusalem, The University of Hong Kong",
Transformer-DRILL Base,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural,28.1,2019,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural#code,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural/review/?hl=9805,Deep Residual Output Layers for Neural Language Generation,No,,Idiap Research Institute,
CMLM+LAT+4 iterations,https://paperswithcode.com/paper/incorporating-a-local-translation-mechanism,27.35,2020,https://paperswithcode.com/paper/incorporating-a-local-translation-mechanism#code,https://paperswithcode.com/paper/incorporating-a-local-translation-mechanism/review/?hl=21692,Incorporating a Local Translation Mechanism into Non-autoregressive Translation,No,,Carnegie Mellon University,
Transformer Base,https://paperswithcode.com/paper/attention-is-all-you-need,27.3,2017,https://paperswithcode.com/paper/attention-is-all-you-need#code,https://paperswithcode.com/paper/attention-is-all-you-need/review/?hl=113,Attention Is All You Need,No,"Google, Google Brain",University of Toronto,
ResMLP-12,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,26.8,2021,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34918,ResMLP: Feedforward networks for image classification with data-efficient training,No,"Facebook AI Research, Valeo","Sorbonne University, Inria",
ConvS2S,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning,26.4,2017,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#code,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#results,Convolutional Sequence to Sequence Learning,No,Facebook AI Research,,
ResMLP-6,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,26.4,2021,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34917,ResMLP: Feedforward networks for image classification with data-efficient training,No,"Facebook AI Research, Valeo","Sorbonne University, Inria",
GNMT+RL,https://paperswithcode.com/paper/googles-neural-machine-translation-system,26.3,2016,https://paperswithcode.com/paper/googles-neural-machine-translation-system#code,https://paperswithcode.com/paper/googles-neural-machine-translation-system/review/?hl=388,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,No,Google,,
SliceNet,https://paperswithcode.com/paper/depthwise-separable-convolutions-for-neural,26.1,2017,https://paperswithcode.com/paper/depthwise-separable-convolutions-for-neural#code,https://paperswithcode.com/paper/depthwise-separable-convolutions-for-neural/review/?hl=393,Depthwise Separable Convolutions for Neural Machine Translation,No,Google Brain,University of Toronto,
MoE,https://paperswithcode.com/paper/outrageously-large-neural-networks-the,26.03,2017,https://paperswithcode.com/paper/outrageously-large-neural-networks-the#code,https://paperswithcode.com/paper/outrageously-large-neural-networks-the/review/?hl=114,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,No,Google Brain,Jagiellonian University,
DenseNMT,https://paperswithcode.com/paper/dense-information-flow-for-neural-machine,25.52,2018,https://paperswithcode.com/paper/dense-information-flow-for-neural-machine#code,https://paperswithcode.com/paper/dense-information-flow-for-neural-machine/review/?hl=6214,Dense Information Flow for Neural Machine Translation,No,Microsoft Asia,"UT Austin, Peking University",
CMLM+LAT+1 iterations,https://paperswithcode.com/paper/incorporating-a-local-translation-mechanism,25.2,2020,https://paperswithcode.com/paper/incorporating-a-local-translation-mechanism#code,https://paperswithcode.com/paper/incorporating-a-local-translation-mechanism/review/?hl=21693,Incorporating a Local Translation Mechanism into Non-autoregressive Translation,No,,Carnegie Mellon University,
ConvS2S,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning,25.16,2017,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#code,https://paperswithcode.com/paper/convolutional-sequence-to-sequence-learning#results,Convolutional Sequence to Sequence Learning,No,Facebook AI Research,,
ByteNet,https://paperswithcode.com/paper/neural-machine-translation-in-linear-time,23.75,2016,https://paperswithcode.com/paper/neural-machine-translation-in-linear-time#code,https://paperswithcode.com/paper/neural-machine-translation-in-linear-time/review/?hl=389,Neural Machine Translation in Linear Time,No,DeepMind,,
FlowSeq-large,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional,23.64,2019,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional#code,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional/review/?hl=7021,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,No,Facebook AI Research,Carnegie Mellon University,
FlowSeq-large,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional,23.14,2019,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional#code,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional/review/?hl=7022,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,No,Facebook AI Research,Carnegie Mellon University,
FlowSeq-large,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional,22.94,2019,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional#code,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional/review/?hl=7023,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,No,Facebook AI Research,Carnegie Mellon University,
Denoising autoencoders,https://paperswithcode.com/paper/deterministic-non-autoregressive-neural,21.54,2018,https://paperswithcode.com/paper/deterministic-non-autoregressive-neural#code,https://paperswithcode.com/paper/deterministic-non-autoregressive-neural/review/?hl=2862,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,No,,New York University,
RNN Enc-Dec Att,https://paperswithcode.com/paper/effective-approaches-to-attention-based,20.9,2015,https://paperswithcode.com/paper/effective-approaches-to-attention-based#code,https://paperswithcode.com/paper/effective-approaches-to-attention-based/review/?hl=19117,Effective Approaches to Attention-based Neural Machine Translation,No,,Stanford University,
FlowSeq-large,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional,20.85,2019,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional#code,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional/review/?hl=7012,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,No,Facebook AI Research,Carnegie Mellon University,
Deep-Att,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward,20.7,2016,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward#code,https://paperswithcode.com/paper/deep-recurrent-models-with-fast-forward/review/?hl=387,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,No,Baidu Inc.,,
Phrase Based MT,https://paperswithcode.com/paper/edinburghs-syntax-based-systems-at-wmt-2015,20.7,2015,,https://paperswithcode.com/paper/edinburghs-syntax-based-systems-at-wmt-2015#results,Edinburgh's Syntax-Based Systems at WMT 2015,No,,"University of Edinburgh, Johns Hopkins University",
PBSMT + NMT,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine,20.23,2018,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine#code,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine/review/?hl=2795,Phrase-Based & Neural Unsupervised Machine Translation,No,Facebook AI Research,Sorbonne University,
NAT +FT + NPD,https://paperswithcode.com/paper/non-autoregressive-neural-machine-translation-1,19.17,2017,https://paperswithcode.com/paper/non-autoregressive-neural-machine-translation-1#code,https://paperswithcode.com/paper/non-autoregressive-neural-machine-translation-1/review/?hl=2838,Non-Autoregressive Neural Machine Translation,No,Salesforce,The University of Hong Kong,
FlowSeq-base,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional,18.55,2019,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional#code,https://paperswithcode.com/paper/flowseq-non-autoregressive-conditional/review/?hl=7013,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,No,Facebook AI Research,Carnegie Mellon University,
Seq-KD + Seq-Inter + Word-KD,https://paperswithcode.com/paper/sequence-level-knowledge-distillation,18.5,2016,https://paperswithcode.com/paper/sequence-level-knowledge-distillation#code,https://paperswithcode.com/paper/sequence-level-knowledge-distillation/review/?hl=2802,Sequence-Level Knowledge Distillation,No,,Harvard University,
Unsupervised PBSMT,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine,17.94,2018,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine#code,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine/review/?hl=2793,Phrase-Based & Neural Unsupervised Machine Translation,No,Facebook AI Research,Sorbonne University,
NSE-NSE,https://paperswithcode.com/paper/neural-semantic-encoders,17.9,2016,https://paperswithcode.com/paper/neural-semantic-encoders#code,https://paperswithcode.com/paper/neural-semantic-encoders/review/?hl=386,Neural Semantic Encoders,No,,University of Massachusetts Amherst,
Unsupervised NMT + Transformer,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine,17.16,2018,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine#code,https://paperswithcode.com/paper/phrase-based-neural-unsupervised-machine/review/?hl=2794,Phrase-Based & Neural Unsupervised Machine Translation,No,Facebook AI Research,Sorbonne University,
SMT + iterative backtranslation,https://paperswithcode.com/paper/unsupervised-statistical-machine-translation,14.08,2018,https://paperswithcode.com/paper/unsupervised-statistical-machine-translation#code,https://paperswithcode.com/paper/unsupervised-statistical-machine-translation/review/?hl=2831,Unsupervised Statistical Machine Translation,No,,University of the Basque Country,
Reverse RNN Enc-Dec,https://paperswithcode.com/paper/effective-approaches-to-attention-based,14,2015,https://paperswithcode.com/paper/effective-approaches-to-attention-based#code,https://paperswithcode.com/paper/effective-approaches-to-attention-based/review/?hl=19116,Effective Approaches to Attention-based Neural Machine Translation,No,,Stanford University,
RNN Enc-Dec,https://paperswithcode.com/paper/effective-approaches-to-attention-based,11.3,2015,https://paperswithcode.com/paper/effective-approaches-to-attention-based#code,https://paperswithcode.com/paper/effective-approaches-to-attention-based/review/?hl=19115,Effective Approaches to Attention-based Neural Machine Translation,No,,Stanford University,
MAT,https://paperswithcode.com/paper/multi-branch-attentive-transformer,29.9,2020,https://paperswithcode.com/paper/multi-branch-attentive-transformer#code,https://paperswithcode.com/paper/multi-branch-attentive-transformer/review/?hl=17480,Multi-branch Attentive Transformer,No,Microsoft Asia,University of Science and Technology China,