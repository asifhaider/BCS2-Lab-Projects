text-center,paper-title,paper-title href,small,metric-report,metric-report 2,ionicon,meta-information,github href,paper href,meta-information 2,metric-report 3,badge
1,GPT-3,https://paperswithcode.com/paper/language-models-are-few-shot-learners,(Zero-Shot),20.5,175000M,Checkmark,Language Models are Few-Shot Learners,https://paperswithcode.com/paper/language-models-are-few-shot-learners#code,https://paperswithcode.com/paper/language-models-are-few-shot-learners/review/?hl=16792,2020,,
2,BERT-Large-CAS,https://paperswithcode.com/paper/190409408,,31.3,36.1,Checkmark,Language Models with Transformers,https://paperswithcode.com/paper/190409408#code,https://paperswithcode.com/paper/190409408/review/?hl=18142,2019,395M,
3,GPT-2,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask,,35.76,1542M,Checkmark,Language Models are Unsupervised Multitask Learners,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#code,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#results,2019,,
4,Mogrifier LSTM + dynamic eval,https://paperswithcode.com/paper/mogrifier-lstm,,44.9,44.8,Close,Mogrifier LSTM,https://paperswithcode.com/paper/mogrifier-lstm#code,https://paperswithcode.com/paper/mogrifier-lstm/review/?hl=16863,2019,24M,LSTM
5,adversarial + AWD-LSTM-MoS + dynamic eval,https://paperswithcode.com/paper/improving-neural-language-modeling-via,,46.01,46.63,Close,Improving Neural Language Modeling via Adversarial Training,https://paperswithcode.com/paper/improving-neural-language-modeling-via#code,https://paperswithcode.com/paper/improving-neural-language-modeling-via/review/?hl=6160,2019,22M,LSTM
6,GL-LWGC + AWD-MoS-LSTM + dynamic eval,https://paperswithcode.com/paper/gradual-learning-of-recurrent-neural-networks,,46.34,46.64,Close,Gradual Learning of Recurrent Neural Networks,https://paperswithcode.com/paper/gradual-learning-of-recurrent-neural-networks#code,https://paperswithcode.com/paper/gradual-learning-of-recurrent-neural-networks/review/?hl=19297,2018,26M,LSTM
7,FRAGE + AWD-LSTM-MoS + dynamic eval,https://paperswithcode.com/paper/frage-frequency-agnostic-word-representation,,46.54,47.38,Close,FRAGE: Frequency-Agnostic Word Representation,https://paperswithcode.com/paper/frage-frequency-agnostic-word-representation#code,https://paperswithcode.com/paper/frage-frequency-agnostic-word-representation/review/?hl=3477,2018,22M,LSTM
8,AWD-LSTM-DOC x5,https://paperswithcode.com/paper/direct-output-connection-for-a-high-rank,,47.17,48.63,Close,Direct Output Connection for a High-Rank Language Model,https://paperswithcode.com/paper/direct-output-connection-for-a-high-rank#code,https://paperswithcode.com/paper/direct-output-connection-for-a-high-rank/review/?hl=218,2018,185M,LSTM
9,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,https://paperswithcode.com/paper/improved-language-modeling-by-decoding-the,,47.3,48.0,Close,Improved Language Modeling by Decoding the Past,,https://paperswithcode.com/paper/improved-language-modeling-by-decoding-the/review/?hl=3957,2018,22M,LSTM
10,AWD-LSTM-MoS + dynamic eval,https://paperswithcode.com/paper/breaking-the-softmax-bottleneck-a-high-rank,,47.69,48.33,Close,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,https://paperswithcode.com/paper/breaking-the-softmax-bottleneck-a-high-rank#code,https://paperswithcode.com/paper/breaking-the-softmax-bottleneck-a-high-rank/review/?hl=219,2017,22M,LSTM
11,AWD-LSTM-DRILL + dynamic eval,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural,,49.4,49.5,Close,Deep Residual Output Layers for Neural Language Generation,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural#code,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural/review/?hl=9801,2019,24M,LSTM
12,Dense IndRNN+dynamic eval,https://paperswithcode.com/paper/deep-independently-recurrent-neural-network,,50.97,,Close,Deep Independently Recurrent Neural Network (IndRNN),https://paperswithcode.com/paper/deep-independently-recurrent-neural-network#code,https://paperswithcode.com/paper/deep-independently-recurrent-neural-network/review/?hl=8317,2019,,
13,AWD-LSTM + dynamic eval,https://paperswithcode.com/paper/dynamic-evaluation-of-neural-sequence-models,,51.1,51.6,Close,Dynamic Evaluation of Neural Sequence Models,https://paperswithcode.com/paper/dynamic-evaluation-of-neural-sequence-models#code,https://paperswithcode.com/paper/dynamic-evaluation-of-neural-sequence-models/review/?hl=220,2017,24M,LSTM
14,AWD-LSTM-DOC + Partial Shuffle,https://paperswithcode.com/paper/partially-shuffling-the-training-data-to-1,,52.0,53.79,Close,Partially Shuffling the Training Data to Improve Language Models,https://paperswithcode.com/paper/partially-shuffling-the-training-data-to-1#code,https://paperswithcode.com/paper/partially-shuffling-the-training-data-to-1/review/?hl=4287,2019,23M,LSTM
15,AWD-LSTM-DOC,https://paperswithcode.com/paper/direct-output-connection-for-a-high-rank,,52.38,54.12,Close,Direct Output Connection for a High-Rank Language Model,https://paperswithcode.com/paper/direct-output-connection-for-a-high-rank#code,https://paperswithcode.com/paper/direct-output-connection-for-a-high-rank/review/?hl=222,2018,23M,LSTM
16,AWD-LSTM + continuous cache pointer,https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language,,52.8,53.9,Close,Regularizing and Optimizing LSTM Language Models,https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language#code,https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language/review/?hl=221,2017,24M,LSTM
17,AWD-LSTM-MoS + Partial Shuffle,https://paperswithcode.com/paper/partially-shuffling-the-training-data-to-1,,53.92,55.89,Close,Partially Shuffling the Training Data to Improve Language Models,https://paperswithcode.com/paper/partially-shuffling-the-training-data-to-1#code,https://paperswithcode.com/paper/partially-shuffling-the-training-data-to-1/review/?hl=4311,2019,22M,LSTM
18,Trellis Network,https://paperswithcode.com/paper/trellis-networks-for-sequence-modeling,,54.19,,Close,Trellis Networks for Sequence Modeling,https://paperswithcode.com/paper/trellis-networks-for-sequence-modeling#code,https://paperswithcode.com/paper/trellis-networks-for-sequence-modeling/review/?hl=4329,2018,,
19,AWD-LSTM-MoS,https://paperswithcode.com/paper/breaking-the-softmax-bottleneck-a-high-rank,,54.44,56.54,Close,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,https://paperswithcode.com/paper/breaking-the-softmax-bottleneck-a-high-rank#code,https://paperswithcode.com/paper/breaking-the-softmax-bottleneck-a-high-rank/review/?hl=223,2017,22M,LSTM
20,AWD-FWM Schlag et al.,https://paperswithcode.com/paper/learning-associative-inference-using-fast-1,(2020),54.48,56.76,Close,Learning Associative Inference Using Fast Weight Memory,https://paperswithcode.com/paper/learning-associative-inference-using-fast-1#code,https://paperswithcode.com/paper/learning-associative-inference-using-fast-1/review/?hl=39176,2020,24M,
21,Transformer-XL,https://paperswithcode.com/paper/transformer-xl-attentive-language-models,,54.55,56.72,Close,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://paperswithcode.com/paper/transformer-xl-attentive-language-models#code,https://paperswithcode.com/paper/transformer-xl-attentive-language-models/review/?hl=224,2019,24M,
22,Transformer-XL + AutoDropout,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to,,54.9,58.1,Close,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to#code,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to/review/?hl=24599,2021,,
23,2-layer skip-LSTM + dropout tuning,https://paperswithcode.com/paper/pushing-the-bounds-of-dropout,,55.3,57.1,Close,Pushing the bounds of dropout,https://paperswithcode.com/paper/pushing-the-bounds-of-dropout#code,https://paperswithcode.com/paper/pushing-the-bounds-of-dropout/review/?hl=3425,2018,24M,LSTM
24,AWD-LSTM-DRILL,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural,,55.7,58.2,Close,Deep Residual Output Layers for Neural Language Generation,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural#code,https://paperswithcode.com/paper/deep-residual-output-layers-for-neural/review/?hl=9802,2019,24M,LSTM
25,Differentiable NAS,https://paperswithcode.com/paper/darts-differentiable-architecture-search,,56.1,58.3,Close,DARTS: Differentiable Architecture Search,https://paperswithcode.com/paper/darts-differentiable-architecture-search#code,https://paperswithcode.com/paper/darts-differentiable-architecture-search/review/?hl=3426,2018,23M,
26,Dense IndRNN,https://paperswithcode.com/paper/deep-independently-recurrent-neural-network,,56.37,,Close,Deep Independently Recurrent Neural Network (IndRNN),https://paperswithcode.com/paper/deep-independently-recurrent-neural-network#code,https://paperswithcode.com/paper/deep-independently-recurrent-neural-network/review/?hl=8318,2019,,
27,AWD-LSTM 3-layer with Fraternal dropout,https://paperswithcode.com/paper/fraternal-dropout,,56.8,58.9,Close,Fraternal Dropout,https://paperswithcode.com/paper/fraternal-dropout#code,https://paperswithcode.com/paper/fraternal-dropout/review/?hl=225,2017,24M,LSTM
28,DEQ-TrellisNet,https://paperswithcode.com/paper/deep-equilibrium-models,,57.1,24M,Close,Deep Equilibrium Models,https://paperswithcode.com/paper/deep-equilibrium-models#code,https://paperswithcode.com/paper/deep-equilibrium-models/review/?hl=7247,2019,,
29,AWD-LSTM,https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language,,57.3,60.0,Close,Regularizing and Optimizing LSTM Language Models,https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language#code,https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language/review/?hl=226,2017,24M,LSTM
30,Efficient NAS,https://paperswithcode.com/paper/efficient-neural-architecture-search-via-1,,58.6,60.8,Close,Efficient Neural Architecture Search via Parameter Sharing,https://paperswithcode.com/paper/efficient-neural-architecture-search-via-1#code,https://paperswithcode.com/paper/efficient-neural-architecture-search-via-1/review/?hl=3427,2018,24M,
31,NAS Cell,https://paperswithcode.com/paper/neural-architecture-search-with-reinforcement,,64.0,25M,Close,Neural Architecture Search with Reinforcement Learning,https://paperswithcode.com/paper/neural-architecture-search-with-reinforcement#code,https://paperswithcode.com/paper/neural-architecture-search-with-reinforcement/review/?hl=3428,2016,,
32,Recurrent highway networks,https://paperswithcode.com/paper/recurrent-highway-networks,,65.4,67.9,Close,Recurrent Highway Networks,https://paperswithcode.com/paper/recurrent-highway-networks#code,https://paperswithcode.com/paper/recurrent-highway-networks/review/?hl=3429,2016,23M,
33,Inan et al.,https://paperswithcode.com/paper/tying-word-vectors-and-word-classifiers-a,(2016),66.0,68.1,Close,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,https://paperswithcode.com/paper/tying-word-vectors-and-word-classifiers-a#code,https://paperswithcode.com/paper/tying-word-vectors-and-word-classifiers-a/review/?hl=11120,2016,,
34,Gal & Ghahramani,https://paperswithcode.com/paper/a-theoretically-grounded-application-of,(2016),75.2,77.9,Close,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,https://paperswithcode.com/paper/a-theoretically-grounded-application-of#code,https://paperswithcode.com/paper/a-theoretically-grounded-application-of/review/?hl=11121,2015,,LSTM
35,Zaremba et al.,https://paperswithcode.com/paper/recurrent-neural-network-regularization,(2014),78.4,82.2,Close,Recurrent Neural Network Regularization,https://paperswithcode.com/paper/recurrent-neural-network-regularization#code,https://paperswithcode.com/paper/recurrent-neural-network-regularization/review/?hl=11122,2014,,LSTM
36,LSTM,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic,"(Bai et al., 2018)",78.93,,Close,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic#code,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic/review/?hl=11123,2018,,LSTM
37,Gal & Ghahramani,https://paperswithcode.com/paper/a-theoretically-grounded-application-of,(2016),79.7,81.9,Close,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,https://paperswithcode.com/paper/a-theoretically-grounded-application-of#code,https://paperswithcode.com/paper/a-theoretically-grounded-application-of/review/?hl=11124,2015,,LSTM
38,Zaremba et al.,https://paperswithcode.com/paper/recurrent-neural-network-regularization,(2014),82.7,86.2,Close,Recurrent Neural Network Regularization,https://paperswithcode.com/paper/recurrent-neural-network-regularization#code,https://paperswithcode.com/paper/recurrent-neural-network-regularization/review/?hl=11125,2014,,LSTM
39,R-Transformer,https://paperswithcode.com/paper/r-transformer-recurrent-neural-network,,84.38,,Close,R-Transformer: Recurrent Neural Network Enhanced Transformer,https://paperswithcode.com/paper/r-transformer-recurrent-neural-network#code,https://paperswithcode.com/paper/r-transformer-recurrent-neural-network/review/?hl=5870,2019,,
40,GRU,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic,"(Bai et al., 2018)",92.48,,Close,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic#code,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic/review/?hl=11126,2018,,