text-center,paper-title,paper-title href,metric-report,metric-report 2,ionicon,meta-information,github href,paper href,meta-information 2,badge,small,metric-report 3
1,Megatron-LM,https://paperswithcode.com/paper/megatron-lm-training-multi-billion-parameter,10.81,8300M,Checkmark,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://paperswithcode.com/paper/megatron-lm-training-multi-billion-parameter#code,https://paperswithcode.com/paper/megatron-lm-training-multi-billion-parameter/review/?hl=7720,2019,Transformer,,
2,GLM-XXLarge,https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general,11.33,10000M,Close,All NLP Tasks Are Generation Tasks: A General Pretraining Framework,https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general#code,https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general/review/?hl=39824,2021,,(bidirectional),
3,GLM-XXLarge,https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general,12.22,10000M,Close,All NLP Tasks Are Generation Tasks: A General Pretraining Framework,https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general#code,https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general/review/?hl=39825,2021,,(unidirectional),
4,kNN-LM,https://paperswithcode.com/paper/generalization-through-memorization-nearest,15.79,15.81,Close,Generalization through Memorization: Nearest Neighbor Language Models,https://paperswithcode.com/paper/generalization-through-memorization-nearest#code,https://paperswithcode.com/paper/generalization-through-memorization-nearest/review/?hl=8697,2019,,,247M
5,Routing Transformer,https://paperswithcode.com/paper/efficient-content-based-sparse-attention-with-1,15.8,,Close,Efficient Content-Based Sparse Attention with Routing Transformers,https://paperswithcode.com/paper/efficient-content-based-sparse-attention-with-1#code,https://paperswithcode.com/paper/efficient-content-based-sparse-attention-with-1/review/?hl=17070,2020,Transformer,,
6,Transformer-XL,https://paperswithcode.com/paper/dynamic-evaluation-of-transformer-language,16.4,15.8,Close,Dynamic Evaluation of Transformer Language Models,https://paperswithcode.com/paper/dynamic-evaluation-of-transformer-language#code,https://paperswithcode.com/paper/dynamic-evaluation-of-transformer-language/review/?hl=5037,2019,Transformer,(RMS dynamic eval),257M
7,Transformer-XL,https://paperswithcode.com/paper/dynamic-evaluation-of-transformer-language,17.0,16.3,Close,Dynamic Evaluation of Transformer Language Models,https://paperswithcode.com/paper/dynamic-evaluation-of-transformer-language#code,https://paperswithcode.com/paper/dynamic-evaluation-of-transformer-language/review/?hl=6402,2019,Transformer,(SGD dynamic eval),257M
8,Compressive Transformer,https://paperswithcode.com/paper/compressive-transformers-for-long-range-1,17.1,16.0,Close,Compressive Transformers for Long-Range Sequence Modelling,https://paperswithcode.com/paper/compressive-transformers-for-long-range-1#code,https://paperswithcode.com/paper/compressive-transformers-for-long-range-1/review/?hl=8044,2019,Transformer,"(18L, M=1024)",
9,SRU++ Large,https://paperswithcode.com/paper/when-attention-meets-fast-recurrence-training,17.1,16.4,Close,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,https://paperswithcode.com/paper/when-attention-meets-fast-recurrence-training#code,https://paperswithcode.com/paper/when-attention-meets-fast-recurrence-training/review/?hl=26853,2021,,,234M
10,SegaTransformer-XL,https://paperswithcode.com/paper/segabert-pre-training-of-segment-aware-bert,17.1,257M,Close,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,https://paperswithcode.com/paper/segabert-pre-training-of-segment-aware-bert#code,https://paperswithcode.com/paper/segabert-pre-training-of-segment-aware-bert/review/?hl=26088,2020,Transformer,,
11,Transformer-XL Large + Phrase Induction,https://paperswithcode.com/paper/improving-neural-language-models-by,17.4,257M,Close,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",https://paperswithcode.com/paper/improving-neural-language-models-by#code,https://paperswithcode.com/paper/improving-neural-language-models-by/review/?hl=10213,2019,Transformer,,
12,GPT-2 Full,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask,17.48,1542M,Checkmark,Language Models are Unsupervised Multitask Learners,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#code,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#results,2019,Transformer,,
13,Staged Training,https://paperswithcode.com/paper/shortformer-better-language-modeling-using,17.56,16.89,Close,Shortformer: Better Language Modeling using Shorter Inputs,https://paperswithcode.com/paper/shortformer-better-language-modeling-using#code,https://paperswithcode.com/paper/shortformer-better-language-modeling-using/review/?hl=24707,2020,Transformer,,247M
14,Sandwich Transformer,https://paperswithcode.com/paper/improving-transformer-models-by-reordering,17.96,247M,Close,Improving Transformer Models by Reordering their Sublayers,https://paperswithcode.com/paper/improving-transformer-models-by-reordering#code,https://paperswithcode.com/paper/improving-transformer-models-by-reordering/review/?hl=17478,2019,Transformer,,
15,DIFFQ,https://paperswithcode.com/paper/differentiable-model-compression-via-pseudo,18.0,,Close,Differentiable Model Compression via Pseudo Quantization Noise,https://paperswithcode.com/paper/differentiable-model-compression-via-pseudo#code,https://paperswithcode.com/paper/differentiable-model-compression-via-pseudo/review/?hl=30879,2021,,"(Î»=1, g=16)",
16,Shortformer,https://paperswithcode.com/paper/shortformer-better-language-modeling-using,18.15,17.47,Close,Shortformer: Better Language Modeling using Shorter Inputs,https://paperswithcode.com/paper/shortformer-better-language-modeling-using#code,https://paperswithcode.com/paper/shortformer-better-language-modeling-using/review/?hl=24706,2020,Transformer,,247M
17,Feedback Transformer,https://paperswithcode.com/paper/accessing-higher-level-representations-in,18.2,17.5,Close,Addressing Some Limitations of Transformers with Feedback Memory,https://paperswithcode.com/paper/accessing-higher-level-representations-in#code,https://paperswithcode.com/paper/accessing-higher-level-representations-in/review/?hl=10124,2020,Transformer,(8 layers),139M
18,SRU++ Base,https://paperswithcode.com/paper/when-attention-meets-fast-recurrence-training,18.3,17.5,Close,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,https://paperswithcode.com/paper/when-attention-meets-fast-recurrence-training#code,https://paperswithcode.com/paper/when-attention-meets-fast-recurrence-training/review/?hl=26852,2021,,,148M
19,Transformer-XL Large,https://paperswithcode.com/paper/transformer-xl-attentive-language-models,18.3,18.2,Close,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://paperswithcode.com/paper/transformer-xl-attentive-language-models#code,https://paperswithcode.com/paper/transformer-xl-attentive-language-models/review/?hl=235,2019,Transformer,,257M
20,PAR Transformer Large,https://paperswithcode.com/paper/pay-attention-when-required,18.4,,Close,Pay Attention when Required,https://paperswithcode.com/paper/pay-attention-when-required#code,https://paperswithcode.com/paper/pay-attention-when-required/review/?hl=21024,2020,Transformer,,
21,Transformer,https://paperswithcode.com/paper/adaptive-input-representations-for-neural,18.70,17.97,Close,Adaptive Input Representations for Neural Language Modeling,https://paperswithcode.com/paper/adaptive-input-representations-for-neural#code,https://paperswithcode.com/paper/adaptive-input-representations-for-neural/review/?hl=236,2018,Transformer,(Adaptive inputs),247M
22,T2R + Pretrain,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns,19.6,19,Close,Finetuning Pretrained Transformers into RNNs,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns#code,https://paperswithcode.com/paper/finetuning-pretrained-transformers-into-rnns/review/?hl=28720,2021,,,
23,Subformer,https://paperswithcode.com/paper/subformer-a-parameter-reduced-transformer,20.39,96M,Close,Subformer: A Parameter Reduced Transformer,,https://paperswithcode.com/paper/subformer-a-parameter-reduced-transformer#results,2020,Transformer,,
24,BERT-Large-CAS,https://paperswithcode.com/paper/190409408,20.4,19.6,Close,Language Models with Transformers,https://paperswithcode.com/paper/190409408#code,https://paperswithcode.com/paper/190409408/review/?hl=18131,2019,Transformer,,395M
25,All-attention network,https://paperswithcode.com/paper/augmenting-self-attention-with-persistent,20.6,19.7,Close,Augmenting Self-attention with Persistent Memory,https://paperswithcode.com/paper/augmenting-self-attention-with-persistent#code,https://paperswithcode.com/paper/augmenting-self-attention-with-persistent/review/?hl=6173,2019,,(36 layers),133M
26,S4,https://paperswithcode.com/paper/efficiently-modeling-long-sequences-with-1,21.28,249M,Close,Efficiently Modeling Long Sequences with Structured State Spaces,https://paperswithcode.com/paper/efficiently-modeling-long-sequences-with-1#code,https://paperswithcode.com/paper/efficiently-modeling-long-sequences-with-1/review/?hl=42009,2021,,,
27,GPT-2 Large,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask,22.05,774M,Checkmark,Language Models are Unsupervised Multitask Learners,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#code,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#results,2019,Transformer,,
28,Feedback Transformer,https://paperswithcode.com/paper/accessing-higher-level-representations-in,22.4,21.4,Close,Addressing Some Limitations of Transformers with Feedback Memory,https://paperswithcode.com/paper/accessing-higher-level-representations-in#code,https://paperswithcode.com/paper/accessing-higher-level-representations-in/review/?hl=10125,2020,Transformer,(4 layers),44M
29,PAR Transformer Base,https://paperswithcode.com/paper/pay-attention-when-required,22.7,,Close,Pay Attention when Required,https://paperswithcode.com/paper/pay-attention-when-required#code,https://paperswithcode.com/paper/pay-attention-when-required/review/?hl=21023,2020,,,
30,DEQ-Transformer,https://paperswithcode.com/paper/deep-equilibrium-models,23.2,110M,Close,Deep Equilibrium Models,https://paperswithcode.com/paper/deep-equilibrium-models#code,https://paperswithcode.com/paper/deep-equilibrium-models/review/?hl=11132,2019,,"(medium, adaptive embed)",
31,TaLK Convolutions,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions,23.3,240M,Close,Time-aware Large Kernel Convolutions,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions#code,https://paperswithcode.com/paper/time-aware-large-kernel-convolutions/review/?hl=26089,2020,CNN,,
32,Rfa-Gate-Gaussian-Stateful,https://paperswithcode.com/paper/random-feature-attention-1,23.5,22,Close,Random Feature Attention,,https://paperswithcode.com/paper/random-feature-attention-1/review/?hl=27542,2021,,(Big),
33,Transformer-XL Standard,https://paperswithcode.com/paper/transformer-xl-attentive-language-models,24.0,23.1,Close,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://paperswithcode.com/paper/transformer-xl-attentive-language-models#code,https://paperswithcode.com/paper/transformer-xl-attentive-language-models/review/?hl=237,2019,Transformer,,151M
34,DeLighT,https://paperswithcode.com/paper/delight-very-deep-and-light-weight,24.14,99M,Close,DeLighT: Deep and Light-weight Transformer,https://paperswithcode.com/paper/delight-very-deep-and-light-weight#code,https://paperswithcode.com/paper/delight-very-deep-and-light-weight#results,2020,Transformer,,
35,Transformer-N,https://paperswithcode.com/paper/revisiting-simple-neural-probabilistic,25.2,24.1,Close,Revisiting Simple Neural Probabilistic Language Models,https://paperswithcode.com/paper/revisiting-simple-neural-probabilistic#code,https://paperswithcode.com/paper/revisiting-simple-neural-probabilistic#results,2021,,,148M
36,FNetAR Medium,https://paperswithcode.com/paper/fnetar-mixing-tokens-with-autoregressive,25.81,144.4M,Close,FNetAR: Mixing Tokens with Autoregressive Fourier Transforms,,https://paperswithcode.com/paper/fnetar-mixing-tokens-with-autoregressive/review/?hl=37533,2021,,,
37,GPT-2 Medium,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask,26.37,355M,Checkmark,Language Models are Unsupervised Multitask Learners,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#code,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#results,2019,Transformer,,
38,AdvSoft,https://paperswithcode.com/paper/improving-neural-language-modeling-via,28.0,27.2,Close,Improving Neural Language Modeling via Adversarial Training,https://paperswithcode.com/paper/improving-neural-language-modeling-via#code,https://paperswithcode.com/paper/improving-neural-language-modeling-via/review/?hl=26090,2019,,(+ 4 layer QRNN + dynamic eval),
39,DEQ-TrellisNet,https://paperswithcode.com/paper/deep-equilibrium-models,29.0,180M,Close,Deep Equilibrium Models,https://paperswithcode.com/paper/deep-equilibrium-models#code,https://paperswithcode.com/paper/deep-equilibrium-models/review/?hl=7248,2019,,,
40,Trellis Network,https://paperswithcode.com/paper/trellis-networks-for-sequence-modeling,29.19,,Close,Trellis Networks for Sequence Modeling,https://paperswithcode.com/paper/trellis-networks-for-sequence-modeling#code,https://paperswithcode.com/paper/trellis-networks-for-sequence-modeling/review/?hl=4332,2018,,,
41,LSTM,https://paperswithcode.com/paper/fast-parametric-learning-with-activation,29.2,29.0,Close,Fast Parametric Learning with Activation Memorization,,https://paperswithcode.com/paper/fast-parametric-learning-with-activation/review/?hl=238,2018,LSTM,"(Hebbian, Cache, MbPA)",
42,LSTM,https://paperswithcode.com/paper/fast-parametric-learning-with-activation,29.7,29.9,Close,Fast Parametric Learning with Activation Memorization,,https://paperswithcode.com/paper/fast-parametric-learning-with-activation/review/?hl=6403,2018,LSTM,"(Hebbian, Cache)",
43,Rfa-Gate-Gaussian-Stateful,https://paperswithcode.com/paper/random-feature-attention-1,30.5,29.4,Close,Random Feature Attention,,https://paperswithcode.com/paper/random-feature-attention-1/review/?hl=27541,2021,,(Small),
44,LSTM,https://paperswithcode.com/paper/relational-recurrent-neural-networks,31.6,30.8,Close,Relational recurrent neural networks,https://paperswithcode.com/paper/relational-recurrent-neural-networks#code,https://paperswithcode.com/paper/relational-recurrent-neural-networks/review/?hl=5162,2018,LSTM,(RMC),
45,DEQ-Transformer,https://paperswithcode.com/paper/deep-equilibrium-models,32.4,138M,Close,Deep Equilibrium Models,https://paperswithcode.com/paper/deep-equilibrium-models#code,https://paperswithcode.com/paper/deep-equilibrium-models/review/?hl=7250,2019,Transformer,(small),
46,AWD-LSTM-MoS + ATOI,https://paperswithcode.com/paper/alleviating-sequence-information-loss-with,32.85,31.92,Close,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,https://paperswithcode.com/paper/alleviating-sequence-information-loss-with#code,https://paperswithcode.com/paper/alleviating-sequence-information-loss-with/review/?hl=26091,2019,LSTM,,
47,4 layer QRNN,https://paperswithcode.com/paper/an-analysis-of-neural-language-modeling-at,33.0,32.0,Close,An Analysis of Neural Language Modeling at Multiple Scales,https://paperswithcode.com/paper/an-analysis-of-neural-language-modeling-at#code,https://paperswithcode.com/paper/an-analysis-of-neural-language-modeling-at/review/?hl=3412,2018,,,151M
48,LSTM,https://paperswithcode.com/paper/fast-parametric-learning-with-activation,34.3,34.1,Close,Fast Parametric Learning with Activation Memorization,,https://paperswithcode.com/paper/fast-parametric-learning-with-activation/review/?hl=239,2018,LSTM,(Hebbian),
49,LSTM,https://paperswithcode.com/paper/fast-parametric-learning-with-activation,36.4,36.0,Close,Fast Parametric Learning with Activation Memorization,,https://paperswithcode.com/paper/fast-parametric-learning-with-activation/review/?hl=240,2018,LSTM,,
50,GCNN-14,https://paperswithcode.com/paper/language-modeling-with-gated-convolutional,37.2,-,Close,Language Modeling with Gated Convolutional Networks,https://paperswithcode.com/paper/language-modeling-with-gated-convolutional#code,https://paperswithcode.com/paper/language-modeling-with-gated-convolutional/review/?hl=241,2016,CNN,,
51,GPT-2 Small,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask,37.50,124M,Checkmark,Language Models are Unsupervised Multitask Learners,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#code,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#results,2019,,,
52,Neural cache model,https://paperswithcode.com/paper/improving-neural-language-models-with-a,40.8,,Close,Improving Neural Language Models with a Continuous Cache,https://paperswithcode.com/paper/improving-neural-language-models-with-a#code,https://paperswithcode.com/paper/improving-neural-language-models-with-a/review/?hl=242,2016,,"(size = 2,000)",
53,Neural cache model,https://paperswithcode.com/paper/improving-neural-language-models-with-a,44.8,,Close,Improving Neural Language Models with a Continuous Cache,https://paperswithcode.com/paper/improving-neural-language-models-with-a#code,https://paperswithcode.com/paper/improving-neural-language-models-with-a/review/?hl=6405,2016,,(size = 100),
54,GCNN-8,https://paperswithcode.com/paper/language-modeling-with-gated-convolutional,44.9,,Close,Language Modeling with Gated Convolutional Networks,https://paperswithcode.com/paper/language-modeling-with-gated-convolutional#code,https://paperswithcode.com/paper/language-modeling-with-gated-convolutional/review/?hl=6404,2016,CNN,,
55,TCN,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic,45.19,,Close,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic#code,https://paperswithcode.com/paper/an-empirical-evaluation-of-generic/review/?hl=5784,2018,CNN,,
56,Temporal CNN,https://paperswithcode.com/paper/convolutional-sequence-modeling-revisited,45.2,-,Close,Convolutional Sequence Modeling Revisited,,https://paperswithcode.com/paper/convolutional-sequence-modeling-revisited#results,2018,CNN,,
57,LSTM,https://paperswithcode.com/paper/improving-neural-language-models-with-a,48.7,,Close,Improving Neural Language Models with a Continuous Cache,https://paperswithcode.com/paper/improving-neural-language-models-with-a#code,https://paperswithcode.com/paper/improving-neural-language-models-with-a/review/?hl=244,2016,LSTM,,
58,Transformer,https://paperswithcode.com/paper/on-the-adequacy-of-untuned-warmup-for,19.5,,Close,On the adequacy of untuned warmup for adaptive optimization,https://paperswithcode.com/paper/on-the-adequacy-of-untuned-warmup-for#code,https://paperswithcode.com/paper/on-the-adequacy-of-untuned-warmup-for#results,2019,Transformer,(Adaptive inputs),
59,LSTM,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture,52.73,,Close,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture#code,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture/review/?hl=16990,2020,LSTM,,
60,GRU,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture,53.78,,Close,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture#code,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture/review/?hl=16992,2020,GRU,,
61,Decay RNN,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture,76.67,,Close,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture#code,https://paperswithcode.com/paper/how-much-complexity-does-an-rnn-architecture/review/?hl=16991,2020,,,
62,Gopher,https://paperswithcode.com/paper/scaling-language-models-methods-analysis-1,0.566,,Close,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",,https://paperswithcode.com/paper/scaling-language-models-methods-analysis-1#results,2021,,,