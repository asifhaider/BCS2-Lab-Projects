Model,Paper Link,Accuracy,Extra Training Data,Title,Github Href,Paper Href,Year,Parameters,Industry Affiliation,Academia Affiliation,Country
EffNet-L2,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1,96.08,Yes,Sharpness-Aware Minimization for Efficiently Improving Generalization,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1#code,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1/review/?hl=21318,2020,,"BlueShift, Google ",,
SWIN-L + ML-Decoder,https://paperswithcode.com/paper/ml-decoder-scalable-and-versatile,95.1,Yes,ML-Decoder: Scalable and Versatile Classification Head,https://paperswithcode.com/paper/ml-decoder-scalable-and-versatile#code,https://paperswithcode.com/paper/ml-decoder-scalable-and-versatile/review/?hl=44547,2021,,Alibaba,Damo Academy,
ViT-H/14,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1,94.55±0.04,Yes,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1#code,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=28277,2020,,Google Brain,,
ViT-B-16,https://paperswithcode.com/paper/imagenet-21k-pretraining-for-the-masses,94.2,Yes,ImageNet-21K Pretraining for the Masses,https://paperswithcode.com/paper/imagenet-21k-pretraining-for-the-masses#code,https://paperswithcode.com/paper/imagenet-21k-pretraining-for-the-masses/review/?hl=30597,2021,,Alibaba,Damo Academy,
CvT-W24,https://paperswithcode.com/paper/cvt-introducing-convolutions-to-vision,94.09,Yes,CvT: Introducing Convolutions to Vision Transformers,https://paperswithcode.com/paper/cvt-introducing-convolutions-to-vision#code,https://paperswithcode.com/paper/cvt-introducing-convolutions-to-vision/review/?hl=29256,2021,,Microsoft Cloud AI,McGill University,
ViT-L/16,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1,93.90±0.05,Yes,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1#code,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=28278,2020,,Google Brain,,
BiT-L,https://paperswithcode.com/paper/large-scale-learning-of-general-visual,93.51,Yes,Big Transfer (BiT): General Visual Representation Learning,https://paperswithcode.com/paper/large-scale-learning-of-general-visual#code,https://paperswithcode.com/paper/large-scale-learning-of-general-visual/review/?hl=9480,2019,,Google Brain,,Switzerland
CaiT-M-36 U 224,https://paperswithcode.com/paper/going-deeper-with-image-transformers,93.1,Yes,Going deeper with Image Transformers,https://paperswithcode.com/paper/going-deeper-with-image-transformers#code,https://paperswithcode.com/paper/going-deeper-with-image-transformers/review/?hl=29444,2021,,Facebook AI Research,Sorbonne University,
TResNet-L-V2,https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated,92.6,Yes,TResNet: High Performance GPU-Dedicated Architecture,https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated#code,https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated/review/?hl=24240,2020,,Alibaba,,
EfficientNetV2-L,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster,92.3,Yes,EfficientNetV2: Smaller Models and Faster Training,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster#code,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29458,2021,,Google Brain,,
EfficientNetV2-M,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster,92.2,Yes,EfficientNetV2: Smaller Models and Faster Training,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster#code,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29454,2021,,Google Brain,,
BiT-M,https://paperswithcode.com/paper/large-scale-learning-of-general-visual,92.17,Yes,Big Transfer (BiT): General Visual Representation Learning,https://paperswithcode.com/paper/large-scale-learning-of-general-visual#code,https://paperswithcode.com/paper/large-scale-learning-of-general-visual/review/?hl=9484,2019,,Google Brain,,Switzerland
CeiT-S,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,91.8,Yes,Incorporating Convolution Designs into Visual Transformers,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28513,2021,,SenseTime,"Nanyang Technological University, HKUST",
CeiT-S,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,91.8,Yes,Incorporating Convolution Designs into Visual Transformers,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28520,2021,,SenseTime,"Nanyang Technological University, HKUST",
EfficientNet-B7,https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for,91.7,Yes,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for#code,https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for/review/?hl=11227,2019,64M,Google Brain,,
EfficientNetV2-S,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster,91.5,Yes,EfficientNetV2: Smaller Models and Faster Training,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster#code,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29450,2021,,Google Brain,,
GPIPE,https://paperswithcode.com/paper/gpipe-efficient-training-of-giant-neural,91.3,Yes,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://paperswithcode.com/paper/gpipe-efficient-training-of-giant-neural#code,https://paperswithcode.com/paper/gpipe-efficient-training-of-giant-neural/review/?hl=3083,2018,,Google,,
AutoFormer-S | 384,https://paperswithcode.com/paper/autoformer-searching-transformers-for-visual,91.1,Yes,AutoFormer: Searching Transformers for Visual Recognition,https://paperswithcode.com/paper/autoformer-searching-transformers-for-visual#code,https://paperswithcode.com/paper/autoformer-searching-transformers-for-visual/review/?hl=37556,2021,23M,Microsoft Asia,Stony Brook University,
TNT-B,https://paperswithcode.com/paper/transformer-in-transformer,91.1,Yes,Transformer in Transformer,https://paperswithcode.com/paper/transformer-in-transformer#code,https://paperswithcode.com/paper/transformer-in-transformer/review/?hl=27420,2021,65.6M,Huawei Inc.,"University of Chinese Academy of Sciences, University of Macau",
DeiT-B,https://paperswithcode.com/paper/training-data-efficient-image-transformers,90.8,Yes,Training data-efficient image transformers & distillation through attention,https://paperswithcode.com/paper/training-data-efficient-image-transformers#code,https://paperswithcode.com/paper/training-data-efficient-image-transformers/review/?hl=23831,2020,86M,Facebook AI Research,Sorbonne University,
GFNet-H-B,https://paperswithcode.com/paper/global-filter-networks-for-image,90.3,Yes,Global Filter Networks for Image Classification,https://paperswithcode.com/paper/global-filter-networks-for-image#code,https://paperswithcode.com/paper/global-filter-networks-for-image#results,2021,54M,,"Beijing National Research Center for Information Science and Technology, Tsinghua University",
E2E-3M,https://paperswithcode.com/paper/rethinking-recurrent-neural-networks-and,90.27,Yes,Rethinking Recurrent Neural Networks and Other Improvements for Image Classification,https://paperswithcode.com/paper/rethinking-recurrent-neural-networks-and#code,https://paperswithcode.com/paper/rethinking-recurrent-neural-networks-and/review/?hl=36538,2020,,,University of Coimbra,
PyramidNet-272,https://paperswithcode.com/paper/asam-adaptive-sharpness-aware-minimization,89.9,No,ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks,https://paperswithcode.com/paper/asam-adaptive-sharpness-aware-minimization#code,https://paperswithcode.com/paper/asam-adaptive-sharpness-aware-minimization/review/?hl=32000,2021,,Samsung,,
PyramidNet,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1,89.7,Yes,Sharpness-Aware Minimization for Efficiently Improving Generalization,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1#code,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1/review/?hl=21311,2020,,"BlueShift, Google ",,
DVT,https://paperswithcode.com/paper/not-all-images-are-worth-16x16-words-dynamic,89.63,Yes,Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition,https://paperswithcode.com/paper/not-all-images-are-worth-16x16-words-dynamic#code,https://paperswithcode.com/paper/not-all-images-are-worth-16x16-words-dynamic/review/?hl=34137,2021,,Huawei Inc.,"Beijing Academy of Intelligence, Tsinghua University",
ResMLP-24,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,89.5,Yes,ResMLP: Feedforward networks for image classification with data-efficient training,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34936,2021,,"Facebook AI Research, Valeo","Sorbonne University, Inria",
"PyramidNet-272, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,89.46,No,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,2020,32.8M,,"The Chinese University of Hong Kong, Zhejiang University",
CeiT-T,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,89.4,Yes,Incorporating Convolution Designs into Visual Transformers,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28499,2021,,SenseTime,"Nanyang Technological University, HKUST",
PyramidNet+ShakeDrop,https://paperswithcode.com/paper/autoaugment-learning-augmentation-policies,89.3,No,AutoAugment: Learning Augmentation Policies from Data,https://paperswithcode.com/paper/autoaugment-learning-augmentation-policies#code,https://paperswithcode.com/paper/autoaugment-learning-augmentation-policies/review/?hl=16787,2018,,Google Brain,,
ResNet-152 2x,https://paperswithcode.com/paper/revisiting-resnets-improved-training-and,89.3,Yes,Revisiting ResNets: Improved Training and Scaling Strategies,https://paperswithcode.com/paper/revisiting-resnets-improved-training-and#code,https://paperswithcode.com/paper/revisiting-resnets-improved-training-and/review/?hl=28375,2021,,Google Brain,UC Berkeley,
ViT-B/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,89.1,Yes,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34329,2021,,Google,UCLA,
ConvMLP-M,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for,89.1,Yes,ConvMLP: Hierarchical Convolutional MLPs for Vision,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for#code,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39260,2021,,PicsArt Inc.,"University of Oregon, UIUC",
ConvMLP-L,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for,88.6,Yes,ConvMLP: Hierarchical Convolutional MLPs for Vision,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for#code,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39263,2021,,PicsArt Inc.,"University of Oregon, UIUC",
ResNet-152x4-AGC,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full,88.54,Yes,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full#code,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full/review/?hl=34671,2021,,Helmholtz AI,,
ColorNet,https://paperswithcode.com/paper/colornet-investigating-the-importance-of,88.4,No,ColorNet: Investigating the importance of color spaces for image classification,https://paperswithcode.com/paper/colornet-investigating-the-importance-of#code,https://paperswithcode.com/paper/colornet-investigating-the-importance-of/review/?hl=21467,2019,19M,,Tsinghua University,
PyramidNet+ShakeDrop,https://paperswithcode.com/paper/fast-autoaugment,88.3,No,Fast AutoAugment,https://paperswithcode.com/paper/fast-autoaugment#code,https://paperswithcode.com/paper/fast-autoaugment/review/?hl=9315,2019,,Kakao Brain,"UNIST, University of Montreal",
NAT-M4,https://paperswithcode.com/paper/neural-architecture-transfer,88.3,Yes,Neural Architecture Transfer,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16575,2020,9M,,"Southern University of Science and Technology, Michigan State University",
CeiT-T,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,88,Yes,Incorporating Convolution Designs into Visual Transformers,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28506,2021,,SenseTime,"Nanyang Technological University, HKUST",
ResNet-152-SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,87.8,Yes,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34327,2021,,Google,UCLA,
NAT-M3,https://paperswithcode.com/paper/neural-architecture-transfer,87.7,Yes,Neural Architecture Transfer,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16576,2020,7.8M,,"Southern University of Science and Technology, Michigan State University	",
ViT-S/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,87.6,Yes,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34328,2021,,Google,UCLA,
NAT-M2,https://paperswithcode.com/paper/neural-architecture-transfer,87.5,Yes,Neural Architecture Transfer,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16577,2020,6.4M,,"Southern University of Science and Technology, Michigan State University ",
"DenseNet-BC-190, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,87.44,No,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,2020,26.3M,,"The Chinese University of Hong Kong, Zhejiang University",
ConvMLP-S,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for,87.4,Yes,ConvMLP: Hierarchical Convolutional MLPs for Vision,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for#code,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39257,2021,,PicsArt Inc.,"University of Oregon, UIUC",
ResMLP-12,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,87,Yes,ResMLP: Feedforward networks for image classification with data-efficient training,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34935,2021,,"Facebook AI Research, Valeo","Sorbonne University, Inria",
"WRN-40-10, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,86.9,No,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,2020,,,"The Chinese University of Hong Kong, Zhejiang University",
ResNet50,https://paperswithcode.com/paper/resnet-strikes-back-an-improved-training,86.9,No,ResNet strikes back: An improved training procedure in timm,https://paperswithcode.com/paper/resnet-strikes-back-an-improved-training#code,https://paperswithcode.com/paper/resnet-strikes-back-an-improved-training#results,2021,25M,Facebook AI Research,Sorbonne University,
WRN-28-10 * 3,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple,86.81,No,MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple#code,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple/review/?hl=28413,2021,,"Thales, Valeo",Sorbonne University,
PyramidNet + AA,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial,86.64,No,Regularizing Neural Networks via Adversarial Model Perturbation,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#code,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#results,2020,,,"University of Ottawa, Beihang University",
PyramidNet-200 + Shakedrop + Cutmix + PS-KD,https://paperswithcode.com/paper/self-knowledge-distillation-a-simple-way-for,86.41,No,Self-Knowledge Distillation with Progressive Refinement of Targets,https://paperswithcode.com/paper/self-knowledge-distillation-a-simple-way-for#code,https://paperswithcode.com/paper/self-knowledge-distillation-a-simple-way-for/review/?hl=40509,2020,,LG CNS,Seoul National University of Science and Technology,
Mixer-B/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,86.4,Yes,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34331,2021,,Google,UCLA,
PyramidNet-200 + Shakedrop + Cutmix,https://paperswithcode.com/paper/cutmix-regularization-strategy-to-train,86.19,No,CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features,https://paperswithcode.com/paper/cutmix-regularization-strategy-to-train#code,https://paperswithcode.com/paper/cutmix-regularization-strategy-to-train/review/?hl=8235,2019,,"Naver Corp., Line Plus Corp.",Yonsei University,
MUXNet-m,https://paperswithcode.com/paper/muxconv-information-multiplexing-in,86.1,Yes,MUXConv: Information Multiplexing in Convolutional Neural Networks,https://paperswithcode.com/paper/muxconv-information-multiplexing-in#code,https://paperswithcode.com/paper/muxconv-information-multiplexing-in/review/?hl=16112,2020,2.1M,,Michigan State University,
NAT-M1,https://paperswithcode.com/paper/neural-architecture-transfer,86,Yes,Neural Architecture Transfer,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16578,2020,3.8M,,"Southern University of Science and Technology, Michigan State University ",
WRN-28-10,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple,85.77,No,MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple#code,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple/review/?hl=28409,2021,,"Thales, Valeo",Sorbonne University,
"WRN-28-10, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,85.74,No,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,2020,,,"The Chinese University of Hong Kong, Zhejiang University",
WRN-28-8,https://paperswithcode.com/paper/boosting-discriminative-visual-representation,85.5,No,Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup,https://paperswithcode.com/paper/boosting-discriminative-visual-representation#code,https://paperswithcode.com/paper/boosting-discriminative-visual-representation#results,2021,,,Westlake University,
ASANas,https://paperswithcode.com/paper/improving-neural-architecture-search-image,85.42,No,Improving Neural Architecture Search Image Classifiers via Ensemble Learning,https://paperswithcode.com/paper/improving-neural-architecture-search-image#code,https://paperswithcode.com/paper/improving-neural-architecture-search-image/review/?hl=7842,2019,,Google,,
ResNet-50-SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,85.2,Yes,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34326,2021,,Google,UCLA,
WRN-28-8,https://paperswithcode.com/paper/automix-unveiling-the-power-of-mixup,85.16,No,Unveiling the Power of Mixup for Stronger Classifiers,https://paperswithcode.com/paper/automix-unveiling-the-power-of-mixup#code,https://paperswithcode.com/paper/automix-unveiling-the-power-of-mixup#results,2021,,,"Westlake University, University of Chinese Academy of Sciences",
WRN 28-14,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks,85,No,Neural networks with late-phase weights,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks#code,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks/review/?hl=30225,2020,,,"ETH Zurich, University of Zurich",
EEEA-Net-C,https://paperswithcode.com/paper/eeea-net-an-early-exit-evolutionary-neural,84.98,No,EEEA-Net: An Early Exit Evolutionary Neural Architecture Search,https://paperswithcode.com/paper/eeea-net-an-early-exit-evolutionary-neural#code,https://paperswithcode.com/paper/eeea-net-an-early-exit-evolutionary-neural/review/?hl=38330,2021,,,"University of South Australia, Royal University of Bhutan, Naresuan University",
SENet + ShakeEven + Cutout,https://paperswithcode.com/paper/squeeze-and-excitation-networks,84.59,No,Squeeze-and-Excitation Networks,https://paperswithcode.com/paper/squeeze-and-excitation-networks#code,https://paperswithcode.com/paper/squeeze-and-excitation-networks/review/?hl=2734,2017,,Momenta,University of Oxford,
WRN-28-10 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,84.38,No,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24913,2020,,,"University of Southern California, Purdue University",
PyramidNet-272 + SWA,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and,84.16,No,Averaging Weights Leads to Wider Optima and Better Generalization,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and#code,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and/review/?hl=4159,2018,,Samsung,"Higher School of Economics, Cornell University, Lomonosov Moscow State University",
WRN28-10,https://paperswithcode.com/paper/puzzle-mix-exploiting-saliency-and-local-1,84.05,No,Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup,https://paperswithcode.com/paper/puzzle-mix-exploiting-saliency-and-local-1#code,https://paperswithcode.com/paper/puzzle-mix-exploiting-saliency-and-local-1/review/?hl=19214,2020,,,Seoul National University,
HCGNet-A3,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid,84.04,No,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid#code,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid/review/?hl=39439,2020,11.4M,,University of Chinese Academy of Sciences,
DenseNet-BC-190 + FMix,https://paperswithcode.com/paper/understanding-and-enhancing-mixed-sample-data,83.95,No,FMix: Enhancing Mixed Sample Data Augmentation,https://paperswithcode.com/paper/understanding-and-enhancing-mixed-sample-data#code,https://paperswithcode.com/paper/understanding-and-enhancing-mixed-sample-data/review/?hl=10150,2020,,,University of Southampton,
Grafit,https://paperswithcode.com/paper/grafit-learning-fine-grained-image,83.7,No,Grafit: Learning fine-grained image representations with coarse labels,,https://paperswithcode.com/paper/grafit-learning-fine-grained-image/review/?hl=22084,2020,,Facebook AI Research,Sorbonne University,
HCGNet-A2,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid,83.46,No,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid#code,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid/review/?hl=39440,2019,3.1M,,University of Chinese Academy of Sciences,
Res2NeXt-29,https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone,83.44,No,Res2Net: A New Multi-scale Backbone Architecture,https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone#code,https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone/review/?hl=4745,2019,,,"Nankai University, UC Merced, University of Oxford",
DenseNet-BC-190 + Mixup,https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization,83.2,No,mixup: Beyond Empirical Risk Minimization,https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization#code,https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization/review/?hl=5058,2017,,,MIT,
SSAL-DenseNet 190-40,https://paperswithcode.com/paper/contextual-classification-using-self,83.2,No,Contextual Classification Using Self-Supervised Auxiliary Models for Deep Neural Networks,https://paperswithcode.com/paper/contextual-classification-using-self#code,https://paperswithcode.com/paper/contextual-classification-using-self/review/?hl=24313,2021,,DFKI,,
EnAET,https://paperswithcode.com/paper/enaet-self-trained-ensemble-autoencoding,83.13,No,EnAET: A Self-Trained framework for Semi-Supervised and Supervised Learning with Ensemble Transformations,https://paperswithcode.com/paper/enaet-self-trained-ensemble-autoencoding#code,https://paperswithcode.com/paper/enaet-self-trained-ensemble-autoencoding/review/?hl=8866,2019,,Futurewei,"Purdue University, University of Rochester, ",
WRN 28-10,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks,83.06,No,Neural networks with late-phase weights,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks#code,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks/review/?hl=30220,2020,,,"ETH Zurich, University of Zurich",
Wide ResNet+Cutout+no BN scale/offset learning,https://paperswithcode.com/paper/single-bit-per-weight-deep-convolutional,82.95,No,Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems,https://paperswithcode.com/paper/single-bit-per-weight-deep-convolutional#code,https://paperswithcode.com/paper/single-bit-per-weight-deep-convolutional/review/?hl=6199,2019,,,"University of South Australia, Western Sydney University, UC San Diego",
WRN-16-8 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,82.95,No,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24911,2020,,,"University of Southern California, Purdue University",
DenseNet-BC,https://paperswithcode.com/paper/densely-connected-convolutional-networks,82.82,No,Densely Connected Convolutional Networks,https://paperswithcode.com/paper/densely-connected-convolutional-networks#code,https://paperswithcode.com/paper/densely-connected-convolutional-networks/review/?hl=15852,2016,,Facebook AI Research,"Cornell University, Tsinghua University",
CCT-7/3x1*,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact,82.72,No,Escaping the Big Data Paradigm with Compact Transformers,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact#code,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=38236,2021,,PicsArt Inc.,"University of Oregon, UIUC",
SKNet-29,https://paperswithcode.com/paper/selective-kernel-networks,82.67,No,Selective Kernel Networks,https://paperswithcode.com/paper/selective-kernel-networks#code,https://paperswithcode.com/paper/selective-kernel-networks/review/?hl=11228,2019,,Momenta,"Nanjing University, Nanjing University of Science and Technology, Tsinghua University",
DenseNet,https://paperswithcode.com/paper/densely-connected-convolutional-networks,82.62,No,Densely Connected Convolutional Networks,https://paperswithcode.com/paper/densely-connected-convolutional-networks#code,https://paperswithcode.com/paper/densely-connected-convolutional-networks/review/?hl=2488,2016,,Facebook AI Research,"Cornell University, Tsinghua University",
Shared WRN,https://paperswithcode.com/paper/learning-implicitly-recurrent-cnns-through,82.57,No,Learning Implicitly Recurrent CNNs Through Parameter Sharing,https://paperswithcode.com/paper/learning-implicitly-recurrent-cnns-through#code,https://paperswithcode.com/paper/learning-implicitly-recurrent-cnns-through/review/?hl=8228,2019,,,"TTI-Chicago, University of Chicago",
Transformer local-attention,https://paperswithcode.com/paper/aggregating-nested-transformers,82.56,No,Aggregating Nested Transformers,https://paperswithcode.com/paper/aggregating-nested-transformers#code,https://paperswithcode.com/paper/aggregating-nested-transformers/review/?hl=33772,2021,,"Google AI, Google",Rutgers University,
Mixer-S/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,82.4,No,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34330,2021,,Google,UCLA,
ResNet-18,https://paperswithcode.com/paper/boosting-discriminative-visual-representation,82.3,No,Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup,https://paperswithcode.com/paper/boosting-discriminative-visual-representation#code,https://paperswithcode.com/paper/boosting-discriminative-visual-representation#results,2021,,,Westlake University,
WARN,https://paperswithcode.com/paper/attend-and-rectify-a-gated-attention,82.18,Yes,Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery,https://paperswithcode.com/paper/attend-and-rectify-a-gated-attention#code,https://paperswithcode.com/paper/attend-and-rectify-a-gated-attention/review/?hl=10610,2019,,,Universitat Autonoma de Barcelona,
WRN+SWA,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and,82.15,No,Averaging Weights Leads to Wider Optima and Better Generalization,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and#code,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and/review/?hl=4158,2018,,Samsung,"Higher School of Economics, Cornell University, Lomonosov Moscow State University",
ResNet-18,https://paperswithcode.com/paper/automix-unveiling-the-power-of-mixup,82.04,No,Unveiling the Power of Mixup for Stronger Classifiers,https://paperswithcode.com/paper/automix-unveiling-the-power-of-mixup#code,https://paperswithcode.com/paper/automix-unveiling-the-power-of-mixup#results,2021,,,"Westlake University, University of Chinese Academy of Sciences",
Manifold Mixup,https://paperswithcode.com/paper/manifold-mixup-better-representations-by,81.96,No,Manifold Mixup: Better Representations by Interpolating Hidden States,https://paperswithcode.com/paper/manifold-mixup-better-representations-by#code,https://paperswithcode.com/paper/manifold-mixup-better-representations-by/review/?hl=7836,2018,,Facebook AI Research,"Aalto Univeristy, Mila - Quebec AI Institute, Sharif University of Technology",
HCGNet-A1,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid,81.87,No,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid#code,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid/review/?hl=39441,2019,1.1M,,University of Chinese Academy of Sciences,
Residual Gates + WRN,https://paperswithcode.com/paper/learning-identity-mappings-with-residual,81.73,No,Learning Identity Mappings with Residual Gates,,https://paperswithcode.com/paper/learning-identity-mappings-with-residual/review/?hl=8229,2016,,,Federal University of Rio de Janeiro,
AA-Wide-ResNet,https://paperswithcode.com/paper/190409925,81.6,No,Attention Augmented Convolutional Networks,https://paperswithcode.com/paper/190409925#code,https://paperswithcode.com/paper/190409925/review/?hl=4811,2019,,Google Brain,,
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,81.6,No,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33603,2020,,,Peking University,
Wide ResNet,https://paperswithcode.com/paper/wide-residual-networks,81.15,No,Wide Residual Networks,https://paperswithcode.com/paper/wide-residual-networks#code,https://paperswithcode.com/paper/wide-residual-networks/review/?hl=2486,2016,,,EcoledesPonts Paris Tech,
CoPaNet-R-164,https://paperswithcode.com/paper/deep-competitive-pathway-networks,81.1,No,Deep Competitive Pathway Networks,https://paperswithcode.com/paper/deep-competitive-pathway-networks#code,https://paperswithcode.com/paper/deep-competitive-pathway-networks/review/?hl=2490,2017,,,National Chiao Tung University,
SimpleNetv2,https://paperswithcode.com/paper/towards-principled-design-of-deep,80.29,No,Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet,https://paperswithcode.com/paper/towards-principled-design-of-deep#code,https://paperswithcode.com/paper/towards-principled-design-of-deep/review/?hl=3858,2018,,"Arvenware, Technicolor","University of Bonn, Institute For Research In Fundamental Sciences",
UPANets,https://paperswithcode.com/paper/upanets-learning-from-the-universal-pixel,80.29,No,UPANets: Learning from the Universal Pixel Attention Networks,https://paperswithcode.com/paper/upanets-learning-from-the-universal-pixel#code,https://paperswithcode.com/paper/upanets-learning-from-the-universal-pixel#results,2021,,,"The University of Manchester, Yunnan University,  National Taipei University of Technology, National Chiao Tung University",
ResNet56 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,80.14,No,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24909,2020,,,"University of Southern California, Purdue University",
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,79.99,No,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33601,2020,,,Peking University,
VGG11B,https://paperswithcode.com/paper/training-neural-networks-with-local-error,79.9,No,Training Neural Networks with Local Error Signals,https://paperswithcode.com/paper/training-neural-networks-with-local-error#code,https://paperswithcode.com/paper/training-neural-networks-with-local-error/review/?hl=11229,2019,,Kongsberg Seatex,,
NNCLR,https://paperswithcode.com/paper/with-a-little-help-from-my-friends-nearest,79,No,With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations,https://paperswithcode.com/paper/with-a-little-help-from-my-friends-nearest#code,https://paperswithcode.com/paper/with-a-little-help-from-my-friends-nearest/review/?hl=31445,2021,,"Google, DeepMind",,
PreActResNet18,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial,78.49,No,Regularizing Neural Networks via Adversarial Model Perturbation,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#code,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#results,2020,,,"University of Ottawa, Beihang University",
SimpleNetv1,https://paperswithcode.com/paper/lets-keep-it-simple-using-simple,78.37,No,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",https://paperswithcode.com/paper/lets-keep-it-simple-using-simple#code,https://paperswithcode.com/paper/lets-keep-it-simple-using-simple/review/?hl=3857,2016,,"Technicolor, Sensifai","Islamic Azad University, Institute For Research In Fundamental Sciences",
MobileNetV3-large x1.0,https://paperswithcode.com/paper/2003-13549,77.7,No,Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets,https://paperswithcode.com/paper/2003-13549#code,https://paperswithcode.com/paper/2003-13549/review/?hl=20889,2020,,ZEISS Microscopy,,
CCT-6/3x1,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact,77.31,No,Escaping the Big Data Paradigm with Compact Transformers,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact#code,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=35883,2021,3.17M,PicsArt Inc.,"University of Oregon, UIUC",
ResNet-1001,https://paperswithcode.com/paper/identity-mappings-in-deep-residual-networks,77.3,No,Identity Mappings in Deep Residual Networks,https://paperswithcode.com/paper/identity-mappings-in-deep-residual-networks#code,https://paperswithcode.com/paper/identity-mappings-in-deep-residual-networks/review/?hl=531,2016,,Microsoft,,
Evolution,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers,77,No,Large-Scale Evolution of Image Classifiers,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers#code,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers/review/?hl=533,2017,,"Google, Google Brain",,
DIANet,https://paperswithcode.com/paper/dianet-dense-and-implicit-attention-network,76.98,No,DIANet: Dense-and-Implicit Attention Network,https://paperswithcode.com/paper/dianet-dense-and-implicit-attention-network#code,https://paperswithcode.com/paper/dianet-dense-and-implicit-attention-network/review/?hl=5190,2019,,,"New Oriental AI Research Academy, Northwestern Polytechnical University, National University of Singapore, Purdue University",
LP-BNN,https://paperswithcode.com/paper/encoding-the-latent-posterior-of-bayesian,76.85,No,Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification,https://paperswithcode.com/paper/encoding-the-latent-posterior-of-bayesian#code,https://paperswithcode.com/paper/encoding-the-latent-posterior-of-bayesian/review/?hl=22374,2020,,Valeo,"Institut Polytechnique de Paris, Universite Paris-Saclay, Aix Marseille University",
ResNet-18+MM+FRL,https://paperswithcode.com/paper/towards-class-specific-unit,76.64,No,Learning Class Unique Features in Fine-Grained Visual Classification,,https://paperswithcode.com/paper/towards-class-specific-unit/review/?hl=37893,2020,,Elecholic,"Jinan University, Tsinghua University, University of Toronto, The Chinese University of Hong Kong",
ResNet32 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,76.55,No,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24907,2020,,,"University of Southern California, Purdue University",
MomentumNet,https://paperswithcode.com/paper/momentum-residual-neural-networks,76.38 ± 0.42,No,Momentum Residual Neural Networks,https://paperswithcode.com/paper/momentum-residual-neural-networks#code,https://paperswithcode.com/paper/momentum-residual-neural-networks/review/?hl=31937,2021,,Google Brain,"Ecole Normale, CNRS",
SSCNN,https://paperswithcode.com/paper/spatially-sparse-convolutional-neural,75.7,No,Spatially-sparse convolutional neural networks,https://paperswithcode.com/paper/spatially-sparse-convolutional-neural#code,https://paperswithcode.com/paper/spatially-sparse-convolutional-neural/review/?hl=509,2014,,,University of Warwick,
Exponential Linear Units,https://paperswithcode.com/paper/fast-and-accurate-deep-network-learning-by,75.7,No,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),https://paperswithcode.com/paper/fast-and-accurate-deep-network-learning-by#code,https://paperswithcode.com/paper/fast-and-accurate-deep-network-learning-by/review/?hl=529,2015,,,Johannes Kepler University,
Stochastic Depth,https://paperswithcode.com/paper/deep-networks-with-stochastic-depth,75.42,No,Deep Networks with Stochastic Depth,https://paperswithcode.com/paper/deep-networks-with-stochastic-depth#code,https://paperswithcode.com/paper/deep-networks-with-stochastic-depth/review/?hl=2483,2016,,,"Cornell University, Tsinghua University",
ResNet v2-110,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural,74.41,No,Mish: A Self Regularized Non-Monotonic Activation Function,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural#code,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural#results,2019,,,KIIT,
ResNet20 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,74.14,No,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24905,2020,,,"University of Southern California, Purdue University",
MixMatch,https://paperswithcode.com/paper/mixmatch-a-holistic-approach-to-semi,74.1,No,MixMatch: A Holistic Approach to Semi-Supervised Learning,https://paperswithcode.com/paper/mixmatch-a-holistic-approach-to-semi#code,https://paperswithcode.com/paper/mixmatch-a-holistic-approach-to-semi/review/?hl=11230,2019,,Google,,
Fractional MP,https://paperswithcode.com/paper/fractional-max-pooling,73.6,No,Fractional Max-Pooling,https://paperswithcode.com/paper/fractional-max-pooling#code,https://paperswithcode.com/paper/fractional-max-pooling/review/?hl=513,2014,,,University of Warwick,
ResNet+ELU,https://paperswithcode.com/paper/deep-residual-networks-with-exponential,73.5,No,Deep Residual Networks with Exponential Linear Unit,,https://paperswithcode.com/paper/deep-residual-networks-with-exponential/review/?hl=532,2016,,,VJTI,
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,73,No,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33599,2020,,,Peking University,
SOPCNN,https://paperswithcode.com/paper/stochastic-optimization-of-plain,72.96,No,Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods,,https://paperswithcode.com/paper/stochastic-optimization-of-plain#results,2020,4.25M,,Pace University,
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,72.87,No,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33597,2020,,,Peking University,
Tuned CNN,https://paperswithcode.com/paper/scalable-bayesian-optimization-using-deep,72.6,No,Scalable Bayesian Optimization Using Deep Neural Networks,https://paperswithcode.com/paper/scalable-bayesian-optimization-using-deep#code,https://paperswithcode.com/paper/scalable-bayesian-optimization-using-deep/review/?hl=514,2015,,Intel Corp.,"Harvard University, MIT, University of Toronto, NERSC",
CMsC,https://paperswithcode.com/paper/competitive-multi-scale-convolution,72.4,No,Competitive Multi-scale Convolution,,https://paperswithcode.com/paper/competitive-multi-scale-convolution/review/?hl=525,2015,,,The University of Adelaide,
Fitnet4-LSUV,https://paperswithcode.com/paper/all-you-need-is-a-good-init,72.3,No,All you need is a good init,https://paperswithcode.com/paper/all-you-need-is-a-good-init#code,https://paperswithcode.com/paper/all-you-need-is-a-good-init/review/?hl=528,2015,,,Czech Technical University in Prague,
BNM NiN,https://paperswithcode.com/paper/batch-normalized-maxout-network-in-network,71.1,No,Batch-normalized Maxout Network in Network,https://paperswithcode.com/paper/batch-normalized-maxout-network-in-network#code,https://paperswithcode.com/paper/batch-normalized-maxout-network-in-network/review/?hl=524,2015,,,National Chiao Tung University,
MIM,https://paperswithcode.com/paper/on-the-importance-of-normalisation-layers-in,70.8,No,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,,https://paperswithcode.com/paper/on-the-importance-of-normalisation-layers-in/review/?hl=521,2015,,,The University of Adelaide,
NiN+APL,https://paperswithcode.com/paper/learning-activation-functions-to-improve-deep,69.2,No,Learning Activation Functions to Improve Deep Neural Networks,https://paperswithcode.com/paper/learning-activation-functions-to-improve-deep#code,https://paperswithcode.com/paper/learning-activation-functions-to-improve-deep/review/?hl=512,2014,,Adobe Research,UC Irvine,
SWWAE,https://paperswithcode.com/paper/stacked-what-where-auto-encoders,69.1,No,Stacked What-Where Auto-encoders,https://paperswithcode.com/paper/stacked-what-where-auto-encoders#code,https://paperswithcode.com/paper/stacked-what-where-auto-encoders/review/?hl=523,2015,,,New York University,
NiN+Superclass+CDJ,https://paperswithcode.com/paper/deep-convolutional-decision-jungle-for-image,69,No,Deep Convolutional Decision Jungle for Image Classification,,https://paperswithcode.com/paper/deep-convolutional-decision-jungle-for-image#results,2017,,,University of Luebeck,
Spectral Representations for Convolutional Neural Networks,https://paperswithcode.com/paper/spectral-representations-for-convolutional,68.4,No,Spectral Representations for Convolutional Neural Networks,,https://paperswithcode.com/paper/spectral-representations-for-convolutional/review/?hl=526,2015,,,"Harvard University, MIT",
ReActNet-18,https://paperswithcode.com/paper/bnn-bn-training-binary-neural-networks,68.34,No,"""BNN - BN = ?"": Training Binary Neural Networks without Batch Normalization",https://paperswithcode.com/paper/bnn-bn-training-binary-neural-networks#code,https://paperswithcode.com/paper/bnn-bn-training-binary-neural-networks/review/?hl=30309,2021,,,"Cornell University, Carnegie Mellon University, UT Austin, University of Science and Technology China",
VDN,https://paperswithcode.com/paper/training-very-deep-networks,67.8,No,Training Very Deep Networks,https://paperswithcode.com/paper/training-very-deep-networks#code,https://paperswithcode.com/paper/training-very-deep-networks/review/?hl=527,2015,,,IDSIA,
DCNN+GFE,https://paperswithcode.com/paper/deep-convolutional-neural-networks-as-generic,67.7,No,Deep Convolutional Neural Networks as Generic Feature Extractors,,https://paperswithcode.com/paper/deep-convolutional-neural-networks-as-generic#results,2017,,,University of Luebeck,
Tree+Max-Avg pooling,https://paperswithcode.com/paper/generalizing-pooling-functions-in,67.6,No,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",https://paperswithcode.com/paper/generalizing-pooling-functions-in#code,https://paperswithcode.com/paper/generalizing-pooling-functions-in/review/?hl=522,2015,,,UC San Diego,
HD-CNN,https://paperswithcode.com/paper/hd-cnn-hierarchical-deep-convolutional-neural,67.4,No,HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition,https://paperswithcode.com/paper/hd-cnn-hierarchical-deep-convolutional-neural#code,https://paperswithcode.com/paper/hd-cnn-hierarchical-deep-convolutional-neural/review/?hl=517,2014,,eBay Inc.,"UIUC, Carnegie Mellon University, The University of Hong Kong",
Universum Prescription,https://paperswithcode.com/paper/universum-prescription-regularization-using,67.2,No,Universum Prescription: Regularization using Unlabeled Data,,https://paperswithcode.com/paper/universum-prescription-regularization-using/review/?hl=530,2015,,,New York University,
ACN,https://paperswithcode.com/paper/striving-for-simplicity-the-all-convolutional,66.3,No,Striving for Simplicity: The All Convolutional Net,https://paperswithcode.com/paper/striving-for-simplicity-the-all-convolutional#code,https://paperswithcode.com/paper/striving-for-simplicity-the-all-convolutional/review/?hl=511,2014,,,University of Freiburg,
DSN,https://paperswithcode.com/paper/deeply-supervised-nets,65.4,No,Deeply-Supervised Nets,https://paperswithcode.com/paper/deeply-supervised-nets#code,https://paperswithcode.com/paper/deeply-supervised-nets/review/?hl=508,2014,,Microsoft,UC San Diego,
NiN,https://paperswithcode.com/paper/network-in-network,64.3,No,Network In Network,https://paperswithcode.com/paper/network-in-network#code,https://paperswithcode.com/paper/network-in-network/review/?hl=505,2013,,,National University of Singapore,
Tree Priors,https://paperswithcode.com/paper/discriminative-transfer-learning-with-tree,63.2,No,Discriminative Transfer Learning with Tree-based Priors,,https://paperswithcode.com/paper/discriminative-transfer-learning-with-tree#results,2013,,,University of Toronto,
DNN+Probabilistic Maxout,https://paperswithcode.com/paper/improving-deep-neural-networks-with,61.9,No,Improving Deep Neural Networks with Probabilistic Maxout Units,,https://paperswithcode.com/paper/improving-deep-neural-networks-with/review/?hl=506,2013,,,University of Freiburg,
Maxout Network,https://paperswithcode.com/paper/maxout-networks,61.43,No,Maxout Networks,https://paperswithcode.com/paper/maxout-networks#code,https://paperswithcode.com/paper/maxout-networks/review/?hl=2480,2013,,,University of Montreal,
ResNet20+UnsharpMaskLayer,https://paperswithcode.com/paper/unsharp-masking-layer-injecting-prior,60.36,No,Unsharp Masking Layer: Injecting Prior Knowledge in Convolutional Networks for Image Classification,https://paperswithcode.com/paper/unsharp-masking-layer-injecting-prior#code,https://paperswithcode.com/paper/unsharp-masking-layer-injecting-prior#results,2019,,,Tecnologico de Costa Rica,
RReLU,https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations,59.8,No,Empirical Evaluation of Rectified Activations in Convolutional Network,https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations#code,https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations/review/?hl=520,2015,,,"University of Alberta, HKUST, University of Washington, Carnegie Mellon University",
Stochastic Pooling,https://paperswithcode.com/paper/stochastic-pooling-for-regularization-of-deep,57.5,No,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,https://paperswithcode.com/paper/stochastic-pooling-for-regularization-of-deep#code,https://paperswithcode.com/paper/stochastic-pooling-for-regularization-of-deep/review/?hl=501,2013,,,New York University,
Sign-symmetry,https://paperswithcode.com/paper/how-important-is-weight-symmetry-in,48.75,No,How Important is Weight Symmetry in Backpropagation?,https://paperswithcode.com/paper/how-important-is-weight-symmetry-in#code,https://paperswithcode.com/paper/how-important-is-weight-symmetry-in/review/?hl=20690,2015,,,MIT,
