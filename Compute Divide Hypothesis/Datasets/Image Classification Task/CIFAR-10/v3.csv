model,paper-title href,accuracy,parameters,github href,paper href,paper title,year,Industry Affiliation,Academia Affiliation,Country,Extra Training Data
ViT-H/14,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1,99.50Â±0.06,632M,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1#code,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=20351,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020,Google ,,,Yes
CaiT-M-36 U 224,https://paperswithcode.com/paper/going-deeper-with-image-transformers,99.4,,https://paperswithcode.com/paper/going-deeper-with-image-transformers#code,https://paperswithcode.com/paper/going-deeper-with-image-transformers/review/?hl=29443,Going deeper with Image Transformers,2021,Facebook AI Research,Sorbonne University,,Yes
CvT-W24,https://paperswithcode.com/paper/cvt-introducing-convolutions-to-vision,99.39,,https://paperswithcode.com/paper/cvt-introducing-convolutions-to-vision#code,https://paperswithcode.com/paper/cvt-introducing-convolutions-to-vision/review/?hl=29255,CvT: Introducing Convolutions to Vision Transformers,2021,Microsoft Cloud AI,McGill University,,Yes
BiT-L,https://paperswithcode.com/paper/large-scale-learning-of-general-visual,99.37,,https://paperswithcode.com/paper/large-scale-learning-of-general-visual#code,https://paperswithcode.com/paper/large-scale-learning-of-general-visual/review/?hl=9479,Big Transfer (BiT): General Visual Representation Learning,2019,Google ,,,Yes
CeiT-S,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,99.1,,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28519,Incorporating Convolution Designs into Visual Transformers,2021,SenseTime,"Nanyang Technological University, HKUST",,Yes
AutoFormer-S | 384,https://paperswithcode.com/paper/autoformer-searching-transformers-for-visual,99.1,23M,https://paperswithcode.com/paper/autoformer-searching-transformers-for-visual#code,https://paperswithcode.com/paper/autoformer-searching-transformers-for-visual/review/?hl=37557,AutoFormer: Searching Transformers for Visual Recognition,2021,Microsoft Asia,Stony Brook University,,Yes
TNT-B,https://paperswithcode.com/paper/transformer-in-transformer,99.1,65.6M,https://paperswithcode.com/paper/transformer-in-transformer#code,https://paperswithcode.com/paper/transformer-in-transformer/review/?hl=27421,Transformer in Transformer,2021,Huawei Inc.,"University of Macau, University of Chinese Academy of Sciences",,Yes
DeiT-B,https://paperswithcode.com/paper/training-data-efficient-image-transformers,99.1,86M,https://paperswithcode.com/paper/training-data-efficient-image-transformers#code,https://paperswithcode.com/paper/training-data-efficient-image-transformers/review/?hl=23830,Training data-efficient image transformers & distillation through attention,2020,Facebook AI Research,Sorbonne University,,Yes
EfficientNetV2-L,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster,99.1,121M,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster#code,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29457,EfficientNetV2: Smaller Models and Faster Training,2021,Google Brain,,,Yes
LaNet,https://paperswithcode.com/paper/sample-efficient-neural-architecture-search-1,99.03,44.1M,https://paperswithcode.com/paper/sample-efficient-neural-architecture-search-1#code,https://paperswithcode.com/paper/sample-efficient-neural-architecture-search-1#results,Sample-Efficient Neural Architecture Search by Learning Action Space for Monte Carlo Tree Search,2019,Facebook AI Research,Brown University,,No
GPIPE + transfer learning,https://paperswithcode.com/paper/gpipe-efficient-training-of-giant-neural,99,,https://paperswithcode.com/paper/gpipe-efficient-training-of-giant-neural#code,https://paperswithcode.com/paper/gpipe-efficient-training-of-giant-neural/review/?hl=3082,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,2018,Google ,,,Yes
TResNet-XL,https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated,99,,https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated#code,https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated/review/?hl=10492,TResNet: High Performance GPU-Dedicated Architecture,2020,Amazon,,,Yes
CeiT-S,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,99,,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28512,Incorporating Convolution Designs into Visual Transformers,2021,SenseTime,"Nanyang Technological University, HKUST",,Yes
GFNet-H-B,https://paperswithcode.com/paper/global-filter-networks-for-image,99,54M,https://paperswithcode.com/paper/global-filter-networks-for-image#code,https://paperswithcode.com/paper/global-filter-networks-for-image#results,Global Filter Networks for Image Classification,2021,,"Tsinghua University, Beijing National Research Center for Information Science and Technology",,Yes
EfficientNetV2-M,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster,99,55M,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster#code,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29453,EfficientNetV2: Smaller Models and Faster Training,2021,Google Brain,,,Yes
BiT-M,https://paperswithcode.com/paper/large-scale-learning-of-general-visual,98.91,,https://paperswithcode.com/paper/large-scale-learning-of-general-visual#code,https://paperswithcode.com/paper/large-scale-learning-of-general-visual/review/?hl=9483,Big Transfer (BiT): General Visual Representation Learning,2019,Google Brain,,,Yes
EfficientNet-B7,https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for,98.9,64M,https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for#code,https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for/review/?hl=11231,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,2019,Google Brain,,,Yes
"PyramidNet-272, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,98.71,32.6M,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,2020,,"The Chinese University of Hong Kong, Zhejiang University",,No
ResMLP-24,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,98.7,,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34938,ResMLP: Feedforward networks for image classification with data-efficient training,2021,"Facebook AI Research, Valeo","Sorbonne University, Inria",,Yes
EfficientNetV2-S,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster,98.7,24M,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster#code,https://paperswithcode.com/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29449,EfficientNetV2: Smaller Models and Faster Training,2021,Google Brain,,,Yes
PyramidNet-272,https://paperswithcode.com/paper/asam-adaptive-sharpness-aware-minimization,98.68,26M,https://paperswithcode.com/paper/asam-adaptive-sharpness-aware-minimization#code,https://paperswithcode.com/paper/asam-adaptive-sharpness-aware-minimization/review/?hl=32002,ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks,2021,Samsung,,,No
PyramidNet + ShakeDrop + Fast AA + FMix,https://paperswithcode.com/paper/understanding-and-enhancing-mixed-sample-data,98.64,26.21M,https://paperswithcode.com/paper/understanding-and-enhancing-mixed-sample-data#code,https://paperswithcode.com/paper/understanding-and-enhancing-mixed-sample-data/review/?hl=10148,FMix: Enhancing Mixed Sample Data Augmentation,2020,,University of Southampton,,No
PyramidNet,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1,98.6,,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1#code,https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1/review/?hl=21310,Sharpness-Aware Minimization for Efficiently Improving Generalization,2020,Google ,,,Yes
ConvMLP-M,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for,98.6,,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for#code,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39259,ConvMLP: Hierarchical Convolutional MLPs for Vision,2021,PicsArt Inc.,"UIUC, University of Oregon",,Yes
ConvMLP-L,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for,98.6,,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for#code,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39262,ConvMLP: Hierarchical Convolutional MLPs for Vision,2021,PicsArt Inc.,"UIUC, University of Oregon",,Yes
ViT-B/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,98.6,87M,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34323,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2021,Google ,UCLA,,Yes
DVT,https://paperswithcode.com/paper/not-all-images-are-worth-16x16-words-dynamic,98.53,,https://paperswithcode.com/paper/not-all-images-are-worth-16x16-words-dynamic#code,https://paperswithcode.com/paper/not-all-images-are-worth-16x16-words-dynamic/review/?hl=34136,Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition,2021,Huawei Inc.,"Tsinghua University, Beijing Academy of Intelligence",,Yes
E2E-3M,https://paperswithcode.com/paper/rethinking-recurrent-neural-networks-and,98.52,20M,https://paperswithcode.com/paper/rethinking-recurrent-neural-networks-and#code,https://paperswithcode.com/paper/rethinking-recurrent-neural-networks-and/review/?hl=18984,Rethinking Recurrent Neural Networks and Other Improvements for Image Classification,2020,,University of Coimbra,,No
CeiT-T,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual,98.5,,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual#code,https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual/review/?hl=28498,Incorporating Convolution Designs into Visual Transformers,2021,SenseTime,"Nanyang Technological University, HKUST",,Yes
NAT-M4,https://paperswithcode.com/paper/neural-architecture-transfer,98.4,6.9M,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16571,Neural Architecture Transfer,2020,,"Southern University of Science and Technology, Michigan State University",,Yes
"WRN-40-10, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,98.38,55.9M,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,2020,,"The Chinese University of Hong Kong, Zhejiang University",,No
"WRN-28-10, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,98.32,36.7M,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,2020,,"The Chinese University of Hong Kong, Zhejiang University",,No
"Shake-Shake 26 2x96d, S=4",https://paperswithcode.com/paper/splitnet-divide-and-co-training,98.31,26.3M,https://paperswithcode.com/paper/splitnet-divide-and-co-training#code,https://paperswithcode.com/paper/splitnet-divide-and-co-training#results,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,2020,,"The Chinese University of Hong Kong, Zhejiang University",,No
ResNet50,https://paperswithcode.com/paper/resnet-strikes-back-an-improved-training,98.3,25M,https://paperswithcode.com/paper/resnet-strikes-back-an-improved-training#code,https://paperswithcode.com/paper/resnet-strikes-back-an-improved-training#results,ResNet strikes back: An improved training procedure in timm,2021,Facebook AI Research,Sorbonne University,,No
PyramidNet+ShakeDrop,https://paperswithcode.com/paper/fast-autoaugment,98.3,26.21M,https://paperswithcode.com/paper/fast-autoaugment#code,https://paperswithcode.com/paper/fast-autoaugment/review/?hl=9314,Fast AutoAugment,2019,Kakao Brain,,,No
NoisyDARTS-A-t,https://paperswithcode.com/paper/noisy-differentiable-architecture-search,98.28,,https://paperswithcode.com/paper/noisy-differentiable-architecture-search#code,https://paperswithcode.com/paper/noisy-differentiable-architecture-search/review/?hl=16508,Noisy Differentiable Architecture Search,2020,Xiaomi,,,Yes
LeViT-192,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s,98.2,,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s#code,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29482,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,2021,Facebook AI Research,,,Yes
ResNet-152-SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,98.2,,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34321,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2021,Google ,UCLA,,Yes
ViT-S/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,98.2,,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34322,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2021,Google ,UCLA,,Yes
NAT-M3,https://paperswithcode.com/paper/neural-architecture-transfer,98.2,6.2M,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16572,Neural Architecture Transfer,2020,,"Southern University of Science and Technology, Michigan State University",,Yes
LeViT-256,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s,98.1,,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s#code,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29473,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,2021,Facebook AI Research,,,Yes
ResMLP-12,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image,98.1,,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image#code,https://paperswithcode.com/paper/resmlp-feedforward-networks-for-image/review/?hl=34937,ResMLP: Feedforward networks for image classification with data-efficient training,2021,"Facebook AI Research, Valeo","Sorbonne University, Inria",,Yes
PyramidNet + AA,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial,98.02,27.22M,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#code,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#results,Regularizing Neural Networks via Adversarial Model Perturbation,2020,,"Beihang University, University of Ottawa",,No
EnAET,https://paperswithcode.com/paper/enaet-self-trained-ensemble-autoencoding,98.01,36.5M,https://paperswithcode.com/paper/enaet-self-trained-ensemble-autoencoding#code,https://paperswithcode.com/paper/enaet-self-trained-ensemble-autoencoding/review/?hl=8859,EnAET: A Self-Trained framework for Semi-Supervised and Supervised Learning with Ensemble Transformations,2019,Futurewei,"Purdue University, University of Rochester, ",,No
LeViT-384,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s,98,,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s#code,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29464,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,2021,Facebook AI Research,,,Yes
ConvMLP-S,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for,98,,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for#code,https://paperswithcode.com/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39256,ConvMLP: Hierarchical Convolutional MLPs for Vision,2021,PicsArt Inc.,"UIUC, University of Oregon",,Yes
MUXNet-m,https://paperswithcode.com/paper/muxconv-information-multiplexing-in,98,2.1M,https://paperswithcode.com/paper/muxconv-information-multiplexing-in#code,https://paperswithcode.com/paper/muxconv-information-multiplexing-in/review/?hl=16111,MUXConv: Information Multiplexing in Convolutional Neural Networks,2020,,Michigan State University,,Yes
CCT-7/3x1*,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact,98,3.76M,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact#code,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=38235,Escaping the Big Data Paradigm with Compact Transformers,2021,PicsArt Inc.,"UIUC, University of Oregon",,No
Proxyless-G + c/o,https://paperswithcode.com/paper/proxylessnas-direct-neural-architecture,97.92,5.7M,https://paperswithcode.com/paper/proxylessnas-direct-neural-architecture#code,https://paperswithcode.com/paper/proxylessnas-direct-neural-architecture/review/?hl=4485,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,2018,"Google Brain, Facebook","MIT, Stanford University",,No
NAT-M2,https://paperswithcode.com/paper/neural-architecture-transfer,97.9,4.6M,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16573,Neural Architecture Transfer,2020,,"Southern University of Science and Technology, Michigan State University",,Yes
WRN-28-10+AutoDropout+RandAugment,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to,97.9,36.5M,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to#code,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to/review/?hl=24595,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,2021,Google Brain,Carnegie Mellon University,,No
SENet + ShakeShake + Cutout,https://paperswithcode.com/paper/squeeze-and-excitation-networks,97.88,,https://paperswithcode.com/paper/squeeze-and-excitation-networks#code,https://paperswithcode.com/paper/squeeze-and-excitation-networks/review/?hl=2733,Squeeze-and-Excitation Networks,2017,Momenta,University of Oxford,,No
HCGNet-A3,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid,97.86,11.4M,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid#code,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid/review/?hl=39443,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,2019,,University of Chinese Academy of Sciences,,No
ResNet-152x4-AGC,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full,97.82,,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full#code,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full/review/?hl=34674,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,2021,Helmholtz AI,,,Yes
Mixer-B/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,97.8,,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34325,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2021,Google ,UCLA,,Yes
WRN-28-10,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple,97.73,36.5M,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple#code,https://paperswithcode.com/paper/mixmo-mixing-multiple-inputs-for-multiple/review/?hl=28410,MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks,2021,"Thales, Valeo",Sorbonne University,,No
HCGNet-A2,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid,97.71,3.1M,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid#code,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid/review/?hl=39445,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,2019,,University of Chinese Academy of Sciences,,No
WRN + fixup init + mixup + cutout,https://paperswithcode.com/paper/fixup-initialization-residual-learning,97.7,18M,https://paperswithcode.com/paper/fixup-initialization-residual-learning#code,https://paperswithcode.com/paper/fixup-initialization-residual-learning/review/?hl=3946,Fixup Initialization: Residual Learning Without Normalization,2019,"Google Brain, Facebook","MIT, Stanford University",,No
NoisyDARTS-a,https://paperswithcode.com/paper/noisy-differentiable-architecture-search,97.61,5.5M,https://paperswithcode.com/paper/noisy-differentiable-architecture-search#code,https://paperswithcode.com/paper/noisy-differentiable-architecture-search/review/?hl=16509,Noisy Differentiable Architecture Search,2020,Xiaomi,,,No
LeViT-128,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s,97.6,,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s#code,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29491,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,2021,Facebook AI Research,,,Yes
DenseNet-BC-190 + batchboost,https://paperswithcode.com/paper/batchboost-regularization-for-stabilizing,97.54,25.6M,https://paperswithcode.com/paper/batchboost-regularization-for-stabilizing#code,https://paperswithcode.com/paper/batchboost-regularization-for-stabilizing#results,batchboost: regularization for stabilizing training with resistance to underfitting & overfitting,2020,,Poznan University of Technology,,No
LeViT-128S,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s,97.5,,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s#code,https://paperswithcode.com/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29500,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,2021,Facebook AI Research,,,Yes
Shared WRN,https://paperswithcode.com/paper/learning-implicitly-recurrent-cnns-through,97.47,33.5M,https://paperswithcode.com/paper/learning-implicitly-recurrent-cnns-through#code,https://paperswithcode.com/paper/learning-implicitly-recurrent-cnns-through/review/?hl=8226,Learning Implicitly Recurrent CNNs Through Parameter Sharing,2019,,"TTI-Chicago, University of Chicago",,No
Manifold Mixup WRN 28-10,https://paperswithcode.com/paper/manifold-mixup-better-representations-by,97.45,36.5M,https://paperswithcode.com/paper/manifold-mixup-better-representations-by#code,https://paperswithcode.com/paper/manifold-mixup-better-representations-by/review/?hl=7835,Manifold Mixup: Better Representations by Interpolating Hidden States,2018,Facebook AI Research,"Aalto Univeristy, Mila - Quebec AI Institute, Sharif University of Technology",,No
WRN 28-14,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks,97.45,36.5M,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks#code,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks/review/?hl=30224,Neural networks with late-phase weights,2020,,"ETH Zurich, University of Zurich",,No
WRN-28-10 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,97.42,36.5M,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24912,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,2020,,"University of Southern California, Purdue University",,No
NAT-M1,https://paperswithcode.com/paper/neural-architecture-transfer,97.4,4.3M,https://paperswithcode.com/paper/neural-architecture-transfer#code,https://paperswithcode.com/paper/neural-architecture-transfer/review/?hl=16574,Neural Architecture Transfer,2020,,"Southern University of Science and Technology, Michigan State University",,Yes
ResNet-50-SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,97.4,25M,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34320,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2021,Google ,UCLA,,Yes
DenseNet-BC-190 + Mixup,https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization,97.3,25.6M,https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization#code,https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization/review/?hl=5057,mixup: Beyond Empirical Risk Minimization,2017,Facebook AI Research,MIT,,No
Transformer local-attention,https://paperswithcode.com/paper/aggregating-nested-transformers,97.2,90.1M,https://paperswithcode.com/paper/aggregating-nested-transformers#code,https://paperswithcode.com/paper/aggregating-nested-transformers/review/?hl=33771,Aggregating Nested Transformers,2021,"Google AI, Google",Rutgers University,,No
ShakeShake-2x64d + SWA,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and,97.12,,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and#code,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and/review/?hl=4163,Averaging Weights Leads to Wider Optima and Better Generalization,2018,Samsung,"Lomonosov Moscow State University, Cornell University, Higher School of Economics",,No
PyramidNet-200 + CutMix,https://paperswithcode.com/paper/cutmix-regularization-strategy-to-train,97.12,,https://paperswithcode.com/paper/cutmix-regularization-strategy-to-train#code,https://paperswithcode.com/paper/cutmix-regularization-strategy-to-train/review/?hl=11232,CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features,2019,"Line Plus Corp., Naver Corp.",Yonsei University,,No
WRN-16-8 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,96.87,,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24910,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,2020,,"University of Southern California, Purdue University",,No
ResNet_XnIDR,https://paperswithcode.com/paper/xnodr-and-xnidr-two-accurate-and-fast-fully,96.87,,https://paperswithcode.com/paper/xnodr-and-xnidr-two-accurate-and-fast-fully#code,https://paperswithcode.com/paper/xnodr-and-xnidr-two-accurate-and-fast-fully/review/?hl=43587,XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks,2021,,University of Denver,,No
HCGNet-A1,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid,96.85,1.1M,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid#code,https://paperswithcode.com/paper/gated-convolutional-networks-with-hybrid/review/?hl=39444,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,2019,,University of Chinese Academy of Sciences,,No
WRN 28-10,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks,96.81,,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks#code,https://paperswithcode.com/paper/economical-ensembles-with-hypernetworks/review/?hl=30223,Neural networks with late-phase weights,2020,,"ETH Zurich, University of Zurich",,No
AutoDropout,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to,96.8,,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to#code,https://paperswithcode.com/paper/autodropout-learning-dropout-patterns-to/review/?hl=24592,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,2021,Google Brain,Carnegie Mellon University,,No
WRN-28-10 + SWA,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and,96.79,,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and#code,https://paperswithcode.com/paper/averaging-weights-leads-to-wider-optima-and/review/?hl=4162,Averaging Weights Leads to Wider Optima and Better Generalization,2018,Samsung,"Lomonosov Moscow State University, Cornell University, Higher School of Economics",,No
Wide ResNet+cutout,https://paperswithcode.com/paper/single-bit-per-weight-deep-convolutional,96.71,,https://paperswithcode.com/paper/single-bit-per-weight-deep-convolutional#code,https://paperswithcode.com/paper/single-bit-per-weight-deep-convolutional/review/?hl=6200,Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems,2019,,"UC San Diego, University of South Australia, Western Sydney University",,No
Deep pyramidal residual network,https://paperswithcode.com/paper/deep-pyramidal-residual-networks,96.69,,https://paperswithcode.com/paper/deep-pyramidal-residual-networks#code,https://paperswithcode.com/paper/deep-pyramidal-residual-networks/review/?hl=4492,Deep Pyramidal Residual Networks,2016,,KAIST,,No
CoPaNet-R-164,https://paperswithcode.com/paper/deep-competitive-pathway-networks,96.62,,https://paperswithcode.com/paper/deep-competitive-pathway-networks#code,https://paperswithcode.com/paper/deep-competitive-pathway-networks/review/?hl=2489,Deep Competitive Pathway Networks,2017,,National Chiao Tung University,,No
DenseNet,https://paperswithcode.com/paper/densely-connected-convolutional-networks,96.54,,https://paperswithcode.com/paper/densely-connected-convolutional-networks#code,https://paperswithcode.com/paper/densely-connected-convolutional-networks/review/?hl=2487,Densely Connected Convolutional Networks,2016,Facebook AI Research,"Cornell University, Tsinghua University",,No
SKNet-29,https://paperswithcode.com/paper/selective-kernel-networks,96.53,,https://paperswithcode.com/paper/selective-kernel-networks#code,https://paperswithcode.com/paper/selective-kernel-networks/review/?hl=11233,Selective Kernel Networks,2019,Momenta,"Tsinghua University, Nanjing University of Science and Technology, Nanjing University",,No
Fractional MP,https://paperswithcode.com/paper/fractional-max-pooling,96.5,,https://paperswithcode.com/paper/fractional-max-pooling#code,https://paperswithcode.com/paper/fractional-max-pooling/review/?hl=561,Fractional Max-Pooling,2014,,University of Warwick,,No
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,96.5,,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33602,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,2020,,Peking University,,No
UPANets,https://paperswithcode.com/paper/upanets-learning-from-the-universal-pixel,96.47,,https://paperswithcode.com/paper/upanets-learning-from-the-universal-pixel#code,https://paperswithcode.com/paper/upanets-learning-from-the-universal-pixel#results,UPANets: Learning from the Universal Pixel Attention Networks,2021,,"The University of Manchester, Yunnan University,  National Taipei University of Technology, National Chiao Tung University",,No
Neural Architecture Search,https://paperswithcode.com/paper/neural-architecture-search-with-reinforcement,96.4,,https://paperswithcode.com/paper/neural-architecture-search-with-reinforcement#code,https://paperswithcode.com/paper/neural-architecture-search-with-reinforcement/review/?hl=586,Neural Architecture Search with Reinforcement Learning,2016,Google Brain,,,No
VGG11B,https://paperswithcode.com/paper/training-neural-networks-with-local-error,96.4,,https://paperswithcode.com/paper/training-neural-networks-with-local-error#code,https://paperswithcode.com/paper/training-neural-networks-with-local-error/review/?hl=11234,Training Neural Networks with Local Error Signals,2019,Kongsberg Seatex,,,No
Residual Gates + WRN,https://paperswithcode.com/paper/learning-identity-mappings-with-residual,96.35,,,https://paperswithcode.com/paper/learning-identity-mappings-with-residual/review/?hl=8227,Learning Identity Mappings with Residual Gates,2016,,Federal University of Rio de Janeiro,,No
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,96.32,,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33600,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,2020,,Peking University,,No
SimpleNetv2,https://paperswithcode.com/paper/towards-principled-design-of-deep,96.29,,https://paperswithcode.com/paper/towards-principled-design-of-deep#code,https://paperswithcode.com/paper/towards-principled-design-of-deep/review/?hl=3856,Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet,2018,"Arvenware, Technicolor","University of Bonn, Institute For Research In Fundamental Sciences",,No
ResNet56 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,96.12,,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24908,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,2020,,"University of Southern California, Purdue University",,No
Wide ResNet,https://paperswithcode.com/paper/wide-residual-networks,96.11,,https://paperswithcode.com/paper/wide-residual-networks#code,https://paperswithcode.com/paper/wide-residual-networks/review/?hl=2485,Wide Residual Networks,2016,,"Universitat Paris-Est, EcoledesPonts Paris Tech",,No
Mixer-S/16- SAM,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets,96.1,,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets#code,https://paperswithcode.com/paper/when-vision-transformers-outperform-resnets/review/?hl=34324,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2021,Google ,UCLA,,No
PreActResNet18,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial,96.03,,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#code,https://paperswithcode.com/paper/regularizing-neural-networks-via-adversarial#results,Regularizing Neural Networks via Adversarial Model Perturbation,2020,,"Beihang University, University of Ottawa",,No
ResNet-50x1-ACG,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full,95.78,,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full#code,https://paperswithcode.com/paper/effect-of-large-scale-pre-training-on-full/review/?hl=34673,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,2021,Helmholtz AI,,,No
ACN,https://paperswithcode.com/paper/striving-for-simplicity-the-all-convolutional,95.6,,https://paperswithcode.com/paper/striving-for-simplicity-the-all-convolutional#code,https://paperswithcode.com/paper/striving-for-simplicity-the-all-convolutional/review/?hl=559,Striving for Simplicity: The All Convolutional Net,2014,,University of Freiburg,,No
Evolution ensemble,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers,95.6,,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers#code,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers/review/?hl=587,Large-Scale Evolution of Image Classifiers,2017,"Google Brain, Google ",,,No
SimpleNetv1,https://paperswithcode.com/paper/lets-keep-it-simple-using-simple,95.51,,https://paperswithcode.com/paper/lets-keep-it-simple-using-simple#code,https://paperswithcode.com/paper/lets-keep-it-simple-using-simple/review/?hl=3855,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",2016,"Technicolor, Sensifai","Islamic Azad University, Institute For Research In Fundamental Sciences",,No
ResNet-1001,https://paperswithcode.com/paper/identity-mappings-in-deep-residual-networks,95.4,,https://paperswithcode.com/paper/identity-mappings-in-deep-residual-networks#code,https://paperswithcode.com/paper/identity-mappings-in-deep-residual-networks/review/?hl=584,Identity Mappings in Deep Residual Networks,2016,Microsoft,,,No
ResNet32 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,95.35,,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24906,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,2020,,"University of Southern California, Purdue University",,No
ResNet-18+MM+FRL,https://paperswithcode.com/paper/towards-class-specific-unit,95.33,,,https://paperswithcode.com/paper/towards-class-specific-unit/review/?hl=37892,Learning Class Unique Features in Fine-Grained Visual Classification,2020,Elecholic,"Jinan University, Tsinghua University, University of Toronto, The Chinese University of Hong Kong",,No
CCT-6/3x1,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact,95.29,3.17M,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact#code,https://paperswithcode.com/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=35882,Escaping the Big Data Paradigm with Compact Transformers,2021,PicsArt Inc.,"UIUC, University of Oregon",,No
MomentumNet,https://paperswithcode.com/paper/momentum-residual-neural-networks,95.18 Â± 0.06,,https://paperswithcode.com/paper/momentum-residual-neural-networks#code,https://paperswithcode.com/paper/momentum-residual-neural-networks/review/?hl=31936,Momentum Residual Neural Networks,2021,Google Brain,"Ecole Normale, CNRS",,No
SRM-ResNet-56,https://paperswithcode.com/paper/srm-a-style-based-recalibration-module-for,95.05,,https://paperswithcode.com/paper/srm-a-style-based-recalibration-module-for#code,https://paperswithcode.com/paper/srm-a-style-based-recalibration-module-for/review/?hl=5104,SRM : A Style-based Recalibration Module for Convolutional Neural Networks,2019,Lunit Inc.,,,No
MixMatch,https://paperswithcode.com/paper/mixmatch-a-holistic-approach-to-semi,95.05,,https://paperswithcode.com/paper/mixmatch-a-holistic-approach-to-semi#code,https://paperswithcode.com/paper/mixmatch-a-holistic-approach-to-semi/review/?hl=11235,MixMatch: A Holistic Approach to Semi-Supervised Learning,2019,Google ,,,No
WRN-22-8,https://paperswithcode.com/paper/sparse-networks-from-scratch-faster-training,95.04,,https://paperswithcode.com/paper/sparse-networks-from-scratch-faster-training#code,https://paperswithcode.com/paper/sparse-networks-from-scratch-faster-training/review/?hl=11236,Sparse Networks from Scratch: Faster Training without Losing Performance,2019,,University of Washington,,No
LP-BNN,https://paperswithcode.com/paper/encoding-the-latent-posterior-of-bayesian,95.02,,https://paperswithcode.com/paper/encoding-the-latent-posterior-of-bayesian#code,https://paperswithcode.com/paper/encoding-the-latent-posterior-of-bayesian/review/?hl=22373,Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification,2020,Valeo,"Institut Polytechnique de Paris, Universite Paris-Saclay, Aix Marseille University",,No
Prodpoly,https://paperswithcode.com/paper/deep-polynomial-neural-networks,94.9,,https://paperswithcode.com/paper/deep-polynomial-neural-networks#code,https://paperswithcode.com/paper/deep-polynomial-neural-networks/review/?hl=27347,Deep Polynomial Neural Networks,2020,,"University of Athens, EPFL, Imperial College London",,No
Stochastic Depth,https://paperswithcode.com/paper/deep-networks-with-stochastic-depth,94.77,,https://paperswithcode.com/paper/deep-networks-with-stochastic-depth#code,https://paperswithcode.com/paper/deep-networks-with-stochastic-depth/review/?hl=2482,Deep Networks with Stochastic Depth,2016,,"Tsinghua University, Cornell University",,No
VGG-19 with GradInit,https://paperswithcode.com/paper/gradinit-learning-to-initialize-neural,94.71,20.03M,https://paperswithcode.com/paper/gradinit-learning-to-initialize-neural#code,https://paperswithcode.com/paper/gradinit-learning-to-initialize-neural#results,GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training,2021,Google ,University of Maryland CollegePark,,No
ResNet20 with reSGHMC,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange,94.62,,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange#code,https://paperswithcode.com/paper/non-convex-learning-via-replica-exchange/review/?hl=24904,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,2020,,"University of Southern California, Purdue University",,No
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,94.62,,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33598,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,2020,,Peking University,,No
Evolution,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers,94.6,,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers#code,https://paperswithcode.com/paper/large-scale-evolution-of-image-classifiers/review/?hl=588,Large-Scale Evolution of Image Classifiers,2017,"Google Brain, Google ",,,No
RL+NT,https://paperswithcode.com/paper/efficient-architecture-search-by-network,94.6,,https://paperswithcode.com/paper/efficient-architecture-search-by-network#code,https://paperswithcode.com/paper/efficient-architecture-search-by-network/review/?hl=590,Efficient Architecture Search by Network Transformation,2017,,"University College London, Shanghai Jiao Tong University",,No
ResNet+ELU,https://paperswithcode.com/paper/deep-residual-networks-with-exponential,94.4,,,https://paperswithcode.com/paper/deep-residual-networks-with-exponential/review/?hl=585,Deep Residual Networks with Exponential Linear Unit,2016,,VJTI,,No
Deep Complex,https://paperswithcode.com/paper/deep-complex-networks,94.4,,https://paperswithcode.com/paper/deep-complex-networks#code,https://paperswithcode.com/paper/deep-complex-networks/review/?hl=589,Deep Complex Networks,2017,"Element AI, Microsoft","Mila - Quebec AI Institute, Polytechnique Montreal",,No
PDO-eConv,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator,94.35,,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator#code,https://paperswithcode.com/paper/pdo-econvs-partial-differential-operator/review/?hl=33596,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,2020,,Peking University,,No
Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods,https://paperswithcode.com/paper/stochastic-optimization-of-plain,94.29,4.3M,,https://paperswithcode.com/paper/stochastic-optimization-of-plain#results,Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods,2020,,Pace University,,No
Fitnet4-LSUV,https://paperswithcode.com/paper/all-you-need-is-a-good-init,94.2,,https://paperswithcode.com/paper/all-you-need-is-a-good-init#code,https://paperswithcode.com/paper/all-you-need-is-a-good-init/review/?hl=581,All you need is a good init,2015,,Czech Technical University in Prague,,No
ResNet 9 + Mish,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural,94.05,,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural#code,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural#results,Mish: A Self Regularized Non-Monotonic Activation Function,2019,,KIIT,,No
Tree+Max-Avg pooling,https://paperswithcode.com/paper/generalizing-pooling-functions-in,94,,https://paperswithcode.com/paper/generalizing-pooling-functions-in#code,https://paperswithcode.com/paper/generalizing-pooling-functions-in/review/?hl=573,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",2015,,UC San Diego,,No
Standard ACNet,https://paperswithcode.com/paper/adaptively-connected-neural-networks,94,,https://paperswithcode.com/paper/adaptively-connected-neural-networks#code,https://paperswithcode.com/paper/adaptively-connected-neural-networks/review/?hl=11237,Adaptively Connected Neural Networks,2019,,"SunYat-sen University, UCLA",,No
SA quadratic embedding,https://paperswithcode.com/paper/on-the-relationship-between-self-attention-1,93.8,,https://paperswithcode.com/paper/on-the-relationship-between-self-attention-1#code,https://paperswithcode.com/paper/on-the-relationship-between-self-attention-1/review/?hl=11238,On the Relationship between Self-Attention and Convolutional Layers,2019,,EPFL,,No
SSCNN,https://paperswithcode.com/paper/spatially-sparse-convolutional-neural,93.7,,https://paperswithcode.com/paper/spatially-sparse-convolutional-neural#code,https://paperswithcode.com/paper/spatially-sparse-convolutional-neural/review/?hl=555,Spatially-sparse convolutional neural networks,2014,,University of Warwick,,No
NNCLR,https://paperswithcode.com/paper/with-a-little-help-from-my-friends-nearest,93.7,,https://paperswithcode.com/paper/with-a-little-help-from-my-friends-nearest#code,https://paperswithcode.com/paper/with-a-little-help-from-my-friends-nearest/review/?hl=31444,With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations,2021,"Google, DeepMind",,,No
Tuned CNN,https://paperswithcode.com/paper/scalable-bayesian-optimization-using-deep,93.6,,https://paperswithcode.com/paper/scalable-bayesian-optimization-using-deep#code,https://paperswithcode.com/paper/scalable-bayesian-optimization-using-deep/review/?hl=562,Scalable Bayesian Optimization Using Deep Neural Networks,2015,Intel Corp.,"Harvard University, MIT, University of Toronto, NERSC",,No
Exponential Linear Units,https://paperswithcode.com/paper/fast-and-accurate-deep-network-learning-by,93.5,,https://paperswithcode.com/paper/fast-and-accurate-deep-network-learning-by#code,https://paperswithcode.com/paper/fast-and-accurate-deep-network-learning-by/review/?hl=582,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),2015,,Johannes Kepler University,,No
BNM NiN,https://paperswithcode.com/paper/batch-normalized-maxout-network-in-network,93.3,,https://paperswithcode.com/paper/batch-normalized-maxout-network-in-network#code,https://paperswithcode.com/paper/batch-normalized-maxout-network-in-network/review/?hl=575,Batch-normalized Maxout Network in Network,2015,,National Chiao Tung University,,No
Universum Prescription,https://paperswithcode.com/paper/universum-prescription-regularization-using,93.3,,,https://paperswithcode.com/paper/universum-prescription-regularization-using/review/?hl=583,Universum Prescription: Regularization using Unlabeled Data,2015,,New York University,,No
CMsC,https://paperswithcode.com/paper/competitive-multi-scale-convolution,93.1,,,https://paperswithcode.com/paper/competitive-multi-scale-convolution/review/?hl=576,Competitive Multi-scale Convolution,2015,,The University of Adelaide,,No
NiN+APL,https://paperswithcode.com/paper/learning-activation-functions-to-improve-deep,92.5,,https://paperswithcode.com/paper/learning-activation-functions-to-improve-deep#code,https://paperswithcode.com/paper/learning-activation-functions-to-improve-deep/review/?hl=560,Learning Activation Functions to Improve Deep Neural Networks,2014,Adobe Research,UC Irvine,,No
VDN,https://paperswithcode.com/paper/training-very-deep-networks,92.4,,https://paperswithcode.com/paper/training-very-deep-networks#code,https://paperswithcode.com/paper/training-very-deep-networks/review/?hl=577,Training Very Deep Networks,2015,,IDSIA,,No
ResNet,https://paperswithcode.com/paper/a-bregman-learning-framework-for-sparse,92.3,,https://paperswithcode.com/paper/a-bregman-learning-framework-for-sparse#code,https://paperswithcode.com/paper/a-bregman-learning-framework-for-sparse#results,A Bregman Learning Framework for Sparse Neural Networks,2021,,University of Erlangen-Nuremberg,,No
SWWAE,https://paperswithcode.com/paper/stacked-what-where-auto-encoders,92.2,,https://paperswithcode.com/paper/stacked-what-where-auto-encoders#code,https://paperswithcode.com/paper/stacked-what-where-auto-encoders/review/?hl=574,Stacked What-Where Auto-encoders,2015,,New York University,,No
FlexTCN-7,https://paperswithcode.com/paper/flexconv-continuous-kernel-convolutions-with-1,92.2Â±0.1,0.67M,https://paperswithcode.com/paper/flexconv-continuous-kernel-convolutions-with-1#code,https://paperswithcode.com/paper/flexconv-continuous-kernel-convolutions-with-1/review/?hl=40955,FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes,2021,,"Delft University of Technology, University of Amsterdam, Vrije Universiteit Amsterdam",,No
ReActNet-18,https://paperswithcode.com/paper/bnn-bn-training-binary-neural-networks,92.08,,https://paperswithcode.com/paper/bnn-bn-training-binary-neural-networks#code,https://paperswithcode.com/paper/bnn-bn-training-binary-neural-networks/review/?hl=30308,"""BNN - BN = ?"": Training Binary Neural Networks without Batch Normalization",2021,,"University of Science and Technology China, UT Austin, Cornell University, Carnegie Mellon University",,No
ResNet v2-20,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural,92.02,,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural#code,https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural#results,Mish: A Self Regularized Non-Monotonic Activation Function,2019,,KIIT,,No
DSN,https://paperswithcode.com/paper/deeply-supervised-nets,91.8,,https://paperswithcode.com/paper/deeply-supervised-nets#code,https://paperswithcode.com/paper/deeply-supervised-nets/review/?hl=553,Deeply-Supervised Nets,2014,Microsoft,UC San Diego,,No
BinaryConnect,https://paperswithcode.com/paper/binaryconnect-training-deep-neural-networks,91.7,,https://paperswithcode.com/paper/binaryconnect-training-deep-neural-networks#code,https://paperswithcode.com/paper/binaryconnect-training-deep-neural-networks/review/?hl=578,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,2015,,"Polytechnique Montreal, University of Montreal",,No
CLS-GAN,https://paperswithcode.com/paper/loss-sensitive-generative-adversarial,91.7,,https://paperswithcode.com/paper/loss-sensitive-generative-adversarial#code,https://paperswithcode.com/paper/loss-sensitive-generative-adversarial/review/?hl=2075,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,2017,,University of Central Florida,,No
MIM,https://paperswithcode.com/paper/on-the-importance-of-normalisation-layers-in,91.5,,,https://paperswithcode.com/paper/on-the-importance-of-normalisation-layers-in/review/?hl=572,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,2015,,The University of Adelaide,,No
Spectral Representations for Convolutional Neural Networks,https://paperswithcode.com/paper/spectral-representations-for-convolutional,91.4,,,https://paperswithcode.com/paper/spectral-representations-for-convolutional/review/?hl=579,Spectral Representations for Convolutional Neural Networks,2015,,"Harvard University, MIT",,No
RMDL,https://paperswithcode.com/paper/rmdl-random-multimodel-deep-learning-for,91.21,,https://paperswithcode.com/paper/rmdl-random-multimodel-deep-learning-for#code,https://paperswithcode.com/paper/rmdl-random-multimodel-deep-learning-for/review/?hl=11239,RMDL: Random Multimodel Deep Learning for Classification,2018,,University of Virginia,,No
Network in Network,https://paperswithcode.com/paper/network-in-network,91.2,,https://paperswithcode.com/paper/network-in-network#code,https://paperswithcode.com/paper/network-in-network/review/?hl=549,Network In Network,2013,,National University of Singapore,,No
Deep Networks with Internal Selective Attention through Feedback Connections,https://paperswithcode.com/paper/deep-networks-with-internal-selective,90.8,,,https://paperswithcode.com/paper/deep-networks-with-internal-selective/review/?hl=556,Deep Networks with Internal Selective Attention through Feedback Connections,2014,,IDSIA,,No
Maxout Network,https://paperswithcode.com/paper/maxout-networks,90.65,,https://paperswithcode.com/paper/maxout-networks#code,https://paperswithcode.com/paper/maxout-networks/review/?hl=2479,Maxout Networks,2013,,University of Montreal,,No
DNN+Probabilistic Maxout,https://paperswithcode.com/paper/improving-deep-neural-networks-with,90.6,,,https://paperswithcode.com/paper/improving-deep-neural-networks-with/review/?hl=550,Improving Deep Neural Networks with Probabilistic Maxout Units,2013,,University of Freiburg,,No
GP EI,https://paperswithcode.com/paper/practical-bayesian-optimization-of-machine,90.5,,https://paperswithcode.com/paper/practical-bayesian-optimization-of-machine#code,https://paperswithcode.com/paper/practical-bayesian-optimization-of-machine#results,Practical Bayesian Optimization of Machine Learning Algorithms,2012,,"Universite de Sherbrooke, University of Toronto, Harvard University",,No
APAC,https://paperswithcode.com/paper/apac-augmented-pattern-classification-with,89.7,,,https://paperswithcode.com/paper/apac-augmented-pattern-classification-with/review/?hl=563,APAC: Augmented PAttern Classification with Neural Networks,2015,Denso Corp.,,,No
ensemble of 7 models,https://paperswithcode.com/paper/dynamic-routing-between-capsules,89.4,,https://paperswithcode.com/paper/dynamic-routing-between-capsules#code,https://paperswithcode.com/paper/dynamic-routing-between-capsules/review/?hl=10426,Dynamic Routing Between Capsules,2017,Google Brain,,,No
DCNN+GFE,https://paperswithcode.com/paper/deep-convolutional-neural-networks-as-generic,89.1,,,https://paperswithcode.com/paper/deep-convolutional-neural-networks-as-generic#results,Deep Convolutional Neural Networks as Generic Feature Extractors,2017,,University of Luebeck,,No
DCNN,https://paperswithcode.com/paper/imagenet-classification-with-deep,89,,https://paperswithcode.com/paper/imagenet-classification-with-deep#code,https://paperswithcode.com/paper/imagenet-classification-with-deep#results,ImageNet Classification with Deep Convolutional Neural Networks,2012,,University of Toronto,,No
MCDNN,https://paperswithcode.com/paper/multi-column-deep-neural-networks-for-image,88.8,,https://paperswithcode.com/paper/multi-column-deep-neural-networks-for-image#code,https://paperswithcode.com/paper/multi-column-deep-neural-networks-for-image/review/?hl=538,Multi-column Deep Neural Networks for Image Classification,2012,,IDSIA,,No
RReLU,https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations,88.8,,https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations#code,https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations/review/?hl=571,Empirical Evaluation of Rectified Activations in Convolutional Network,2015,,"University of Alberta, University of Washington, Carnegie Mellon University, HKUST",,No
ReNet,https://paperswithcode.com/paper/renet-a-recurrent-neural-network-based,87.7,,https://paperswithcode.com/paper/renet-a-recurrent-neural-network-based#code,https://paperswithcode.com/paper/renet-a-recurrent-neural-network-based/review/?hl=566,ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks,2015,,University of Montreal,,No
An Analysis of Unsupervised Pre-training in Light of Recent Advances,https://paperswithcode.com/paper/an-analysis-of-unsupervised-pre-training-in,86.7,,https://paperswithcode.com/paper/an-analysis-of-unsupervised-pre-training-in#code,https://paperswithcode.com/paper/an-analysis-of-unsupervised-pre-training-in/review/?hl=558,An Analysis of Unsupervised Pre-training in Light of Recent Advances,2014,,UIUC,,No
Stochastic Pooling,https://paperswithcode.com/paper/stochastic-pooling-for-regularization-of-deep,84.9,,https://paperswithcode.com/paper/stochastic-pooling-for-regularization-of-deep#code,https://paperswithcode.com/paper/stochastic-pooling-for-regularization-of-deep/review/?hl=545,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,2013,,New York University,,No
Improving neural networks by preventing co-adaptation of feature detectors,https://paperswithcode.com/paper/improving-neural-networks-by-preventing-co,84.4,,https://paperswithcode.com/paper/improving-neural-networks-by-preventing-co#code,https://paperswithcode.com/paper/improving-neural-networks-by-preventing-co#results,Improving neural networks by preventing co-adaptation of feature detectors,2012,,University of Toronto,,No
CCN,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image,83.36,0.906075M,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image#code,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image/review/?hl=36566,Vision Xformers: Efficient Attention for Image Classification,2021,,IIT Bombay,,No
CvN,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image,83.26,,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image#code,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image/review/?hl=37868,Vision Xformers: Efficient Attention for Image Classification,2021,,IIT Bombay,,No
CvP,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image,83.19,,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image#code,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image/review/?hl=37867,Vision Xformers: Efficient Attention for Image Classification,2021,,IIT Bombay,,No
UL-Hopfield,https://paperswithcode.com/paper/unsupervised-learning-using-pretrained-cnn,83.1,,,https://paperswithcode.com/paper/unsupervised-learning-using-pretrained-cnn/review/?hl=21865,Unsupervised Learning using Pretrained CNN and Associative Memory Bank,2018,,Louisiana State University,,No
DCGAN,https://paperswithcode.com/paper/unsupervised-representation-learning-with-1,82.8,,https://paperswithcode.com/paper/unsupervised-representation-learning-with-1#code,https://paperswithcode.com/paper/unsupervised-representation-learning-with-1/review/?hl=11240,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,2015,"Indico, Facebook AI Research",,,No
Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,https://paperswithcode.com/paper/discriminative-unsupervised-feature-learning-1,82,,,https://paperswithcode.com/paper/discriminative-unsupervised-feature-learning-1#results,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,2014,,University of Freiburg,,No
Sign-symmetry,https://paperswithcode.com/paper/how-important-is-weight-symmetry-in,80.98,,https://paperswithcode.com/paper/how-important-is-weight-symmetry-in#code,https://paperswithcode.com/paper/how-important-is-weight-symmetry-in/review/?hl=20689,How Important is Weight Symmetry in Backpropagation?,2015,,MIT,,Yes
1 Layer K-means,https://paperswithcode.com/paper/unsupervised-representation-learning-with-1,80.6,,https://paperswithcode.com/paper/unsupervised-representation-learning-with-1#code,https://paperswithcode.com/paper/unsupervised-representation-learning-with-1/review/?hl=11241,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,2015,"Indico, Facebook AI Research",,,No
Learning with Recursive Perceptual Representations,https://paperswithcode.com/paper/learning-with-recursive-perceptual,79.7,,,https://paperswithcode.com/paper/learning-with-recursive-perceptual#results,Learning with Recursive Perceptual Representations,2012,,UC Berkeley,,No
PCANet,https://paperswithcode.com/paper/pcanet-a-simple-deep-learning-baseline-for,78.7,,https://paperswithcode.com/paper/pcanet-a-simple-deep-learning-baseline-for#code,https://paperswithcode.com/paper/pcanet-a-simple-deep-learning-baseline-for/review/?hl=552,PCANet: A Simple Deep Learning Baseline for Image Classification?,2014,,"Shanghai Tech University, UIUC",,No
FLSCNN,https://paperswithcode.com/paper/enhanced-image-classification-with-a-fast,75.9,,,https://paperswithcode.com/paper/enhanced-image-classification-with-a-fast/review/?hl=564,Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network,2015,,University of South Australia,,No
Hybrid Vision Nystromformer,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image,75.26,0.623706M,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image#code,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image/review/?hl=36568,Vision Xformers: Efficient Attention for Image Classification,2021,,IIT Bombay,,No
Hybrid PiN,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image,74,0.990298M,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image#code,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image/review/?hl=37870,Vision Xformers: Efficient Attention for Image Classification,2021,,IIT Bombay,,No
Vision Nystromformer,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image,65.06,0.530970M,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image#code,https://paperswithcode.com/paper/vision-xformers-efficient-attention-for-image/review/?hl=36562,Vision Xformers: Efficient Attention for Image Classification,2021,,IIT Bombay,,No
ANODE,https://paperswithcode.com/paper/augmented-neural-odes,60.6,,https://paperswithcode.com/paper/augmented-neural-odes#code,https://paperswithcode.com/paper/augmented-neural-odes/review/?hl=9321,Augmented Neural ODEs,2019,,University of Oxford,,No
ViT-L/16,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1,99.42Â±0.03,307M,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1#code,https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=20352,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020,Google Brain,,,Yes
kEffNet-B0,https://paperswithcode.com/paper/grouped-pointwise-convolutions-significantly,"639,702",,https://paperswithcode.com/paper/grouped-pointwise-convolutions-significantly#code,https://paperswithcode.com/paper/grouped-pointwise-convolutions-significantly#results,Grouped Pointwise Convolutions Significantly Reduces Parameters in EfficientNet,2021,,Universitat Rovira i Virgili,,No
